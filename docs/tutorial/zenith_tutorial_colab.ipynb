{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Tutorial\n",
                "\n",
                "Tutorial lengkap cara menggunakan Zenith ML Optimization Framework.\n",
                "\n",
                "**Chapters:**\n",
                "1. Getting Started\n",
                "2. Basics\n",
                "3. Quantization\n",
                "4. QAT Training\n",
                "5. PyTorch Integration\n",
                "6. Triton Deployment\n",
                "7. Autotuner"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 1: Getting Started"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.1 Install Zenith\n",
                "!rm -rf ZENITH 2>/dev/null\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git\n",
                "%cd ZENITH\n",
                "!pip install -q -e ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.2 Verify Installation\n",
                "import zenith\n",
                "print(f\"Zenith Version: {zenith.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.3 Check Backends\n",
                "from zenith import backends\n",
                "\n",
                "print(f\"CPU Available: {backends.is_cpu_available()}\")\n",
                "print(f\"CUDA Available: {backends.is_cuda_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 2: Basics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 Import Modules\n",
                "import zenith\n",
                "from zenith import backends\n",
                "from zenith.optimization.qat import FakeQuantize, QATConfig\n",
                "from zenith.serving.triton_client import MockTritonClient\n",
                "\n",
                "print(\"All imports successful!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.2 Get Available Backends\n",
                "print(backends.get_available_backends())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 3: Quantization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 Create FakeQuantize\n",
                "from zenith.optimization.qat import FakeQuantize\n",
                "\n",
                "fq = FakeQuantize(num_bits=8, symmetric=True)\n",
                "print(f\"Created FakeQuantize: {fq.num_bits}-bit\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2 Observe Data\n",
                "import numpy as np\n",
                "\n",
                "data = np.random.randn(100).astype(np.float32)\n",
                "fq.observe(data)\n",
                "print(\"Data observed for calibration\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.3 Apply Quantization\n",
                "quantized = fq.forward(data)\n",
                "\n",
                "error = np.mean(np.abs(data - quantized))\n",
                "print(f\"Mean Quantization Error: {error:.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.4 Get Quantization Parameters\n",
                "params = fq.get_quantization_params()\n",
                "print(f\"Scale: {params.scale}\")\n",
                "print(f\"Zero Point: {params.zero_point}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 4: QAT Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 QAT Config\n",
                "from zenith.optimization.qat import QATConfig\n",
                "\n",
                "config = QATConfig(\n",
                "    weight_bits=8,\n",
                "    activation_bits=8,\n",
                "    symmetric_weights=True,\n",
                "    per_channel_weights=True\n",
                ")\n",
                "print(f\"Weight bits: {config.weight_bits}\")\n",
                "print(f\"Activation bits: {config.activation_bits}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 Prepare Model for QAT\n",
                "from zenith.optimization.qat import prepare_model_for_qat\n",
                "\n",
                "layer_names = ['fc1', 'fc2', 'fc3']\n",
                "trainer = prepare_model_for_qat(layer_names, config)\n",
                "print(f\"QAT Trainer created with {len(trainer.modules)} modules\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.3 BatchNorm Folding\n",
                "from zenith.optimization.qat import fold_bn_into_conv\n",
                "\n",
                "weight = np.random.randn(4, 3, 3, 3).astype(np.float32)\n",
                "bias = np.random.randn(4).astype(np.float32)\n",
                "bn_mean = np.random.randn(4).astype(np.float32)\n",
                "bn_var = np.abs(np.random.randn(4)) + 0.1\n",
                "bn_gamma = np.random.randn(4).astype(np.float32)\n",
                "bn_beta = np.random.randn(4).astype(np.float32)\n",
                "\n",
                "folded_w, folded_b = fold_bn_into_conv(weight, bias, bn_mean, bn_var, bn_gamma, bn_beta)\n",
                "print(f\"Folded weight shape: {folded_w.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 5: PyTorch Integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.1 Create PyTorch Model\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SimpleNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(784, 256)\n",
                "        self.fc2 = nn.Linear(256, 10)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.fc2(torch.relu(self.fc1(x)))\n",
                "\n",
                "model = SimpleNet()\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.2 Extract Weights\n",
                "layer_weights = {\n",
                "    'fc1': model.fc1.weight.detach().numpy(),\n",
                "    'fc2': model.fc2.weight.detach().numpy(),\n",
                "}\n",
                "\n",
                "fp32_size = sum(w.nbytes for w in layer_weights.values())\n",
                "print(f\"FP32 Size: {fp32_size / 1024:.2f} KB\")\n",
                "print(f\"INT8 Size: {fp32_size / 4 / 1024:.2f} KB\")\n",
                "print(f\"Reduction: 4.0x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 6: Triton Deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.1 Create Mock Client\n",
                "from zenith.serving.triton_client import MockTritonClient, ModelMetadata\n",
                "\n",
                "client = MockTritonClient(\"localhost:8000\")\n",
                "print(f\"Server Live: {client.is_server_live()}\")\n",
                "print(f\"Server Ready: {client.is_server_ready()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.2 Register Model\n",
                "client.register_model(\n",
                "    \"my_model\",\n",
                "    metadata=ModelMetadata(name=\"my_model\", platform=\"python\")\n",
                ")\n",
                "print(f\"Model Ready: {client.is_model_ready('my_model')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.3 Run Inference\n",
                "from zenith.serving.triton_client import InferenceInput\n",
                "\n",
                "data = np.array([1.0, 2.0, 3.0]).astype(np.float32)\n",
                "inputs = [InferenceInput(name=\"input\", data=data)]\n",
                "\n",
                "result = client.infer(\"my_model\", inputs)\n",
                "print(f\"Success: {result.success}\")\n",
                "print(f\"Latency: {result.latency_ms:.3f} ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Chapter 7: Autotuner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7.1 Define Search Space\n",
                "from zenith.optimization.autotuner import SearchSpace\n",
                "\n",
                "space = SearchSpace(\"matmul_space\")\n",
                "space.define(\"block_size\", [16, 32, 64])\n",
                "space.define(\"num_warps\", [2, 4])\n",
                "print(f\"Search space size: {space.size()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7.2 Benchmark Function\n",
                "def benchmark(config):\n",
                "    return 1000 / (config[\"block_size\"] * config[\"num_warps\"])\n",
                "\n",
                "print(f\"Example: benchmark(block_size=32, num_warps=4) = {benchmark({'block_size': 32, 'num_warps': 4}):.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7.3 Run Tuning\n",
                "from zenith.optimization.autotuner import KernelAutotuner, TuningConfig, GridSearch\n",
                "\n",
                "autotuner = KernelAutotuner(strategy=GridSearch())\n",
                "config = TuningConfig(op_name=\"matmul\", input_shapes=[(512, 512)])\n",
                "\n",
                "best_params, best_time = autotuner.tune(config, space, benchmark, max_trials=6)\n",
                "print(f\"Best Config: {best_params}\")\n",
                "print(f\"Best Time: {best_time:.3f} ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "Anda telah mempelajari:\n",
                "\n",
                "| Chapter | Topic | Status |\n",
                "|---------|-------|--------|\n",
                "| 1 | Getting Started | Completed |\n",
                "| 2 | Basics | Completed |\n",
                "| 3 | Quantization | Completed |\n",
                "| 4 | QAT Training | Completed |\n",
                "| 5 | PyTorch | Completed |\n",
                "| 6 | Triton | Completed |\n",
                "| 7 | Autotuner | Completed |\n",
                "\n",
                "**Selamat! Tutorial selesai!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}