{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Tutorial: ML Optimization Framework\n",
                "\n",
                "Zenith adalah framework optimasi ML yang bekerja sebagai **pelengkap** (bukan pengganti) framework seperti PyTorch dan JAX.\n",
                "\n",
                "**Apa yang akan dipelajari:**\n",
                "1. Instalasi dan setup\n",
                "2. Zenith + PyTorch\n",
                "3. Zenith + JAX\n",
                "4. Fitur advanced (Triton, Auto-tuning)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Getting Started"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.1 Install Zenith dari GitHub\n",
                "!rm -rf ZENITH 2>/dev/null\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git\n",
                "%cd ZENITH\n",
                "!pip install -q -e ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.2 Install dependencies\n",
                "!pip install -q torch jax jaxlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.3 Verify installation\n",
                "import zenith\n",
                "from zenith import backends\n",
                "import numpy as np\n",
                "\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "print(f\"CPU Available: {backends.is_cpu_available()}\")\n",
                "print(f\"CUDA Available: {backends.is_cuda_available()}\")\n",
                "print(\"\\nSetup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Zenith + PyTorch\n",
                "\n",
                "Zenith menyediakan Quantization-Aware Training (QAT) untuk mengoptimasi model PyTorch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 Create PyTorch Model\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SimpleNet(nn.Module):\n",
                "    \"\"\"Simple neural network untuk demo.\"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(784, 256)\n",
                "        self.fc2 = nn.Linear(256, 128)\n",
                "        self.fc3 = nn.Linear(128, 10)\n",
                "        self.relu = nn.ReLU()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.relu(self.fc2(x))\n",
                "        return self.fc3(x)\n",
                "\n",
                "# Create model\n",
                "model = SimpleNet()\n",
                "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.2 Apply Zenith QAT\n",
                "from zenith.optimization.qat import (\n",
                "    QATConfig,\n",
                "    FakeQuantize,\n",
                "    QATTrainer,\n",
                "    prepare_model_for_qat,\n",
                "    convert_qat_to_quantized\n",
                ")\n",
                "\n",
                "# Get layer weights from PyTorch\n",
                "layer_weights = {\n",
                "    'fc1': model.fc1.weight.detach().numpy(),\n",
                "    'fc2': model.fc2.weight.detach().numpy(),\n",
                "    'fc3': model.fc3.weight.detach().numpy(),\n",
                "}\n",
                "\n",
                "# Create QAT config\n",
                "config = QATConfig(\n",
                "    weight_bits=8,\n",
                "    activation_bits=8,\n",
                "    symmetric_weights=True,\n",
                "    per_channel_weights=True\n",
                ")\n",
                "\n",
                "# Prepare for QAT\n",
                "trainer = prepare_model_for_qat(list(layer_weights.keys()), config)\n",
                "print(f\"QAT Trainer created with {len(trainer.modules)} modules\")\n",
                "print(f\"Config: {config.weight_bits}-bit weights, {config.activation_bits}-bit activations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.3 Calibrate QAT with sample data\n",
                "# Generate calibration data\n",
                "calibration_data = np.random.randn(100, 784).astype(np.float32)\n",
                "\n",
                "# Calibrate each layer\n",
                "for layer_name, weights in layer_weights.items():\n",
                "    # Simulate activations\n",
                "    activations = np.random.randn(100, weights.shape[1]).astype(np.float32)\n",
                "    trainer.calibrate(layer_name, weights, activations)\n",
                "\n",
                "print(\"Calibration complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.4 Compare FP32 vs INT8\n",
                "# Get quantization params\n",
                "quantized_weights = convert_qat_to_quantized(trainer, layer_weights)\n",
                "\n",
                "# Calculate size reduction\n",
                "fp32_size = sum(w.nbytes for w in layer_weights.values())\n",
                "int8_size = fp32_size / 4  # 8-bit = 1/4 of 32-bit\n",
                "\n",
                "print(f\"\\nModel Size Comparison:\")\n",
                "print(f\"  FP32: {fp32_size / 1024:.2f} KB\")\n",
                "print(f\"  INT8: {int8_size / 1024:.2f} KB\")\n",
                "print(f\"  Reduction: {fp32_size / int8_size:.1f}x\")\n",
                "\n",
                "# Calculate quantization error\n",
                "for name in layer_weights:\n",
                "    original = layer_weights[name]\n",
                "    quantized = quantized_weights[name]\n",
                "    error = np.mean(np.abs(original - quantized))\n",
                "    snr = 10 * np.log10(np.mean(original**2) / np.mean((original - quantized)**2 + 1e-10))\n",
                "    print(f\"  {name}: MAE={error:.6f}, SNR={snr:.1f}dB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Zenith + JAX\n",
                "\n",
                "Zenith juga dapat digunakan dengan JAX untuk optimasi fungsi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 Create JAX function\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "\n",
                "def jax_mlp(params, x):\n",
                "    \"\"\"Simple MLP in JAX.\"\"\"\n",
                "    for w, b in params[:-1]:\n",
                "        x = jax.nn.relu(jnp.dot(x, w) + b)\n",
                "    w, b = params[-1]\n",
                "    return jnp.dot(x, w) + b\n",
                "\n",
                "# Initialize params\n",
                "key = jax.random.PRNGKey(42)\n",
                "keys = jax.random.split(key, 3)\n",
                "params = [\n",
                "    (jax.random.normal(keys[0], (784, 256)) * 0.01, jnp.zeros(256)),\n",
                "    (jax.random.normal(keys[1], (256, 128)) * 0.01, jnp.zeros(128)),\n",
                "    (jax.random.normal(keys[2], (128, 10)) * 0.01, jnp.zeros(10)),\n",
                "]\n",
                "\n",
                "print(\"JAX MLP created!\")\n",
                "print(f\"Params: {sum(w.size + b.size for w, b in params):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2 Apply Zenith QAT to JAX weights\n",
                "# Extract weights\n",
                "jax_weights = {\n",
                "    f'layer_{i}': np.array(w) for i, (w, b) in enumerate(params)\n",
                "}\n",
                "\n",
                "# Create QAT trainer\n",
                "jax_trainer = prepare_model_for_qat(list(jax_weights.keys()), config)\n",
                "\n",
                "# Calibrate\n",
                "for layer_name, weights in jax_weights.items():\n",
                "    activations = np.random.randn(100, weights.shape[0]).astype(np.float32)\n",
                "    jax_trainer.calibrate(layer_name, weights, activations)\n",
                "\n",
                "# Convert\n",
                "jax_quantized = convert_qat_to_quantized(jax_trainer, jax_weights)\n",
                "\n",
                "print(\"\\nJAX weights quantized!\")\n",
                "for name in jax_weights:\n",
                "    original = jax_weights[name]\n",
                "    quantized = jax_quantized[name]\n",
                "    error = np.mean(np.abs(original - quantized))\n",
                "    print(f\"  {name}: MAE={error:.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.3 Benchmark JAX inference\n",
                "import time\n",
                "\n",
                "# JIT compile\n",
                "jax_mlp_jit = jax.jit(jax_mlp)\n",
                "\n",
                "# Test data\n",
                "test_input = jax.random.normal(jax.random.PRNGKey(0), (32, 784))\n",
                "\n",
                "# Warmup\n",
                "_ = jax_mlp_jit(params, test_input)\n",
                "\n",
                "# Benchmark\n",
                "start = time.perf_counter()\n",
                "for _ in range(100):\n",
                "    _ = jax_mlp_jit(params, test_input)\n",
                "elapsed = (time.perf_counter() - start) * 1000\n",
                "\n",
                "print(f\"\\nJAX Inference Performance:\")\n",
                "print(f\"  100 iterations: {elapsed:.2f} ms\")\n",
                "print(f\"  Per iteration: {elapsed/100:.3f} ms\")\n",
                "print(f\"  Throughput: {100*32/(elapsed/1000):.0f} samples/sec\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4: Advanced Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 Triton Mock Deployment\n",
                "from zenith.serving.triton_client import MockTritonClient, InferenceInput, ModelMetadata\n",
                "\n",
                "# Create mock Triton server\n",
                "client = MockTritonClient(\"localhost:8000\")\n",
                "\n",
                "# Register model with custom handler\n",
                "def inference_handler(inputs):\n",
                "    \"\"\"Model inference logic.\"\"\"\n",
                "    x = inputs[0].data\n",
                "    # Simple forward pass simulation\n",
                "    return {\"output\": np.tanh(x @ np.random.randn(x.shape[-1], 10).astype(np.float32))}\n",
                "\n",
                "client.register_model(\n",
                "    \"my_model\",\n",
                "    metadata=ModelMetadata(name=\"my_model\", platform=\"python\", versions=[\"1\"]),\n",
                "    handler=inference_handler\n",
                ")\n",
                "\n",
                "# Test inference\n",
                "test_input = np.random.randn(1, 784).astype(np.float32)\n",
                "result = client.infer(\"my_model\", [InferenceInput(name=\"input\", data=test_input)])\n",
                "\n",
                "print(f\"Triton Server Status: {client.is_server_ready()}\")\n",
                "print(f\"Inference Success: {result.success}\")\n",
                "print(f\"Output shape: {result.outputs[0].data.shape}\")\n",
                "print(f\"Latency: {result.latency_ms:.3f} ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 Auto-tuning\n",
                "from zenith.optimization.autotuner import (\n",
                "    KernelAutotuner,\n",
                "    SearchSpace,\n",
                "    TuningConfig\n",
                ")\n",
                "\n",
                "# Define search space\n",
                "space = SearchSpace()\n",
                "space.define(\"block_size\", [16, 32, 64, 128])\n",
                "space.define(\"num_warps\", [2, 4, 8])\n",
                "\n",
                "# Define benchmark function\n",
                "def benchmark_kernel(config):\n",
                "    \"\"\"Simulate kernel benchmark.\"\"\"\n",
                "    # Lower is better\n",
                "    return 1000 / (config[\"block_size\"] * config[\"num_warps\"] ** 0.5)\n",
                "\n",
                "# Create autotuner\n",
                "autotuner = KernelAutotuner(search_space=space)\n",
                "\n",
                "# Run tuning\n",
                "config = TuningConfig(op_type=\"matmul\", input_shape=[1024, 1024])\n",
                "result = autotuner.tune(config, benchmark_kernel, max_trials=12)\n",
                "\n",
                "print(f\"\\nAuto-tuning Result:\")\n",
                "print(f\"  Best config: {result.best_config}\")\n",
                "print(f\"  Best time: {result.best_time:.2f}\")\n",
                "print(f\"  Trials: {result.num_trials}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.3 Load Testing\n",
                "import sys\n",
                "sys.path.insert(0, './tests/integration')\n",
                "from triton_load_test import run_mock_load_test\n",
                "\n",
                "# Run load test\n",
                "result = run_mock_load_test(\n",
                "    model_name=\"load_test\",\n",
                "    num_requests=100,\n",
                "    concurrent_workers=10,\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "Anda telah mempelajari:\n",
                "\n",
                "| Feature | Status |\n",
                "|---------|--------|\n",
                "| Zenith Install | Verified |\n",
                "| PyTorch + QAT | Verified |\n",
                "| JAX + QAT | Verified |\n",
                "| Triton Deployment | Verified |\n",
                "| Auto-tuning | Verified |\n",
                "| Load Testing | Verified |\n",
                "\n",
                "### Next Steps\n",
                "- Deploy model ke production dengan Docker/Kubernetes\n",
                "- Gunakan real Triton server\n",
                "- Explore more optimization passes"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}