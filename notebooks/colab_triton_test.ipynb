{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Testing Notebook\n",
                "\n",
                "Testing Triton Integration, Load Testing, dan QAT pada Google Colab.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git\n",
                "%cd ZENITH"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q numpy pytest requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "sys.path.insert(0, './tests')\n",
                "sys.path.insert(0, './tests/integration')\n",
                "\n",
                "import numpy as np\n",
                "print(f\"NumPy version: {np.__version__}\")\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Triton Client Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.serving.triton_client import (\n",
                "    MockTritonClient,\n",
                "    InferenceInput,\n",
                "    ModelMetadata\n",
                ")\n",
                "\n",
                "# Create mock client\n",
                "client = MockTritonClient(\"localhost:8000\")\n",
                "\n",
                "# Register a test model\n",
                "def model_handler(inputs):\n",
                "    return {\"output\": inputs[0].data * 2}\n",
                "\n",
                "client.register_model(\n",
                "    \"test_model\",\n",
                "    metadata=ModelMetadata(name=\"test_model\", platform=\"python\", versions=[\"1\"]),\n",
                "    handler=model_handler\n",
                ")\n",
                "\n",
                "print(f\"Server Live: {client.is_server_live()}\")\n",
                "print(f\"Server Ready: {client.is_server_ready()}\")\n",
                "print(f\"Model Ready: {client.is_model_ready('test_model')}\")\n",
                "print(f\"Models: {client.list_models()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test inference\n",
                "input_data = np.array([[1.0, 2.0, 3.0]], dtype=np.float32)\n",
                "inputs = [InferenceInput(name=\"input\", data=input_data)]\n",
                "\n",
                "result = client.infer(\"test_model\", inputs)\n",
                "\n",
                "print(f\"Success: {result.success}\")\n",
                "print(f\"Model: {result.model_name}\")\n",
                "print(f\"Latency: {result.latency_ms:.3f} ms\")\n",
                "print(f\"Input: {input_data}\")\n",
                "print(f\"Output: {result.get_output('output')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import directly from integration folder\n",
                "from triton_load_test import run_mock_load_test\n",
                "\n",
                "# Run load test with 100 requests\n",
                "result = run_mock_load_test(\n",
                "    model_name=\"load_test_model\",\n",
                "    num_requests=100,\n",
                "    concurrent_workers=10,\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# High concurrency test\n",
                "print(\"\\nHIGH CONCURRENCY TEST\")\n",
                "result = run_mock_load_test(\n",
                "    model_name=\"high_concurrency_model\",\n",
                "    num_requests=500,\n",
                "    concurrent_workers=50,\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. QAT Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.optimization.qat import FakeQuantize, fold_bn_into_conv\n",
                "\n",
                "# Test FakeQuantize\n",
                "fq = FakeQuantize(num_bits=8, symmetric=True)\n",
                "data = np.random.randn(1000).astype(np.float32) * 3\n",
                "fq.observe(data)\n",
                "quantized = fq.forward(data)\n",
                "\n",
                "error = np.abs(data - quantized)\n",
                "print(f\"Max Error: {np.max(error):.6f}\")\n",
                "print(f\"Mean Error: {np.mean(error):.6f}\")\n",
                "print(f\"Scale: {fq.scale}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test BN Folding\n",
                "weight = np.random.randn(4, 3, 3, 3).astype(np.float32)\n",
                "bias = np.random.randn(4).astype(np.float32)\n",
                "bn_mean = np.random.randn(4).astype(np.float32)\n",
                "bn_var = np.abs(np.random.randn(4).astype(np.float32)) + 0.1\n",
                "bn_gamma = np.random.randn(4).astype(np.float32)\n",
                "bn_beta = np.random.randn(4).astype(np.float32)\n",
                "\n",
                "folded_weight, folded_bias = fold_bn_into_conv(weight, bias, bn_mean, bn_var, bn_gamma, bn_beta)\n",
                "print(f\"Weight shape: {folded_weight.shape}\")\n",
                "print(f\"Bias shape: {folded_bias.shape}\")\n",
                "print(\"BN folding success!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run All Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python -m pytest tests/test_triton_integration.py -v --tb=short 2>&1 | head -50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python -m pytest tests/test_qat.py -v --tb=short 2>&1 | head -50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python -m pytest tests/test_triton_backend.py -v --tb=short"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. QAT Benchmark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python benchmarks/qat_benchmark.py --model resnet50 --iterations 50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python benchmarks/qat_benchmark.py --model bert-base --iterations 50"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Done!\n",
                "\n",
                "All tests should pass. Check outputs above."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}