{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Zenith Performance Benchmark - Proper Backend\n",
                "\n",
                "This notebook tests the **real** Zenith optimization pipeline, not a pass-through.\n",
                "\n",
                "**Hardware:** NVIDIA Tesla T4 (Google Colab)\n",
                "**Model:** TinyLlama 1.1B with LoRA fine-tuning"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers peft datasets trl accelerate bitsandbytes\n",
                "# Install Zenith from GitHub (latest with integrations module)\n",
                "!pip install -q git+https://github.com/vibeswithkk/ZENITH.git"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Verify GPU\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Import Zenith - this auto-registers the 'zenith' backend!\n",
                "import zenith\n",
                "print(f\"Zenith Version: {zenith.__version__}\")\n",
                "\n",
                "# Check if backend is registered\n",
                "from zenith.integrations.torch_dynamo import is_registered\n",
                "print(f\"Zenith Backend Registered: {is_registered()}\")\n",
                "\n",
                "# List available backends\n",
                "if hasattr(torch, '_dynamo'):\n",
                "    backends = torch._dynamo.list_backends()\n",
                "    print(f\"Available backends: {backends}\")\n",
                "    assert 'zenith' in backends, \"Zenith backend not registered!\""
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Setup imports\n",
                "import gc\n",
                "import time\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "def clean_memory():\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "    torch.cuda.reset_peak_memory_stats()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Benchmark function\n",
                "def run_benchmark(use_zenith, steps=50, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
                "    mode_name = \"ZENITH (Real Backend)\" if use_zenith else \"PYTORCH (Baseline)\"\n",
                "    print(f\"\\n{'='*20} {mode_name} {'='*20}\")\n",
                "    \n",
                "    clean_memory()\n",
                "    \n",
                "    # Load model\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name,\n",
                "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # Apply LoRA\n",
                "    peft_config = LoraConfig(\n",
                "        task_type=TaskType.CAUSAL_LM,\n",
                "        inference_mode=False,\n",
                "        r=8,\n",
                "        lora_alpha=32,\n",
                "        lora_dropout=0.1\n",
                "    )\n",
                "    model = get_peft_model(model, peft_config)\n",
                "    \n",
                "    # Apply Zenith optimization (REAL backend, not pass-through!)\n",
                "    if use_zenith:\n",
                "        print(\"Applying Zenith optimization via torch.compile...\")\n",
                "        # This uses auto-registered 'zenith' backend from zenith.integrations\n",
                "        model.model = torch.compile(model.model, backend=\"zenith\")\n",
                "        print(\"Zenith compilation complete!\")\n",
                "    \n",
                "    # Dataset\n",
                "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=f\"train[:{steps*2}]\")\n",
                "    def format_prompt(sample):\n",
                "        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
                "    \n",
                "    # Trainer\n",
                "    args = SFTConfig(\n",
                "        output_dir=f\"./results_{mode_name.replace(' ', '_')}\",\n",
                "        per_device_train_batch_size=1,\n",
                "        gradient_accumulation_steps=4,\n",
                "        learning_rate=2e-4,\n",
                "        logging_steps=10,\n",
                "        max_steps=steps,\n",
                "        fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        report_to=\"none\",\n",
                "        packing=False\n",
                "    )\n",
                "    \n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        train_dataset=dataset,\n",
                "        peft_config=peft_config,\n",
                "        args=args,\n",
                "        processing_class=tokenizer,\n",
                "        formatting_func=format_prompt\n",
                "    )\n",
                "    \n",
                "    # Train and time\n",
                "    start = time.time()\n",
                "    trainer.train()\n",
                "    end = time.time()\n",
                "    \n",
                "    # Results\n",
                "    total_time = end - start\n",
                "    peak_mem = torch.cuda.max_memory_allocated() / 1024**3\n",
                "    \n",
                "    results = {\n",
                "        \"mode\": mode_name,\n",
                "        \"total_time\": total_time,\n",
                "        \"peak_vram_gb\": peak_mem,\n",
                "        \"steps\": steps\n",
                "    }\n",
                "    \n",
                "    print(f\"Total Time: {total_time:.2f}s\")\n",
                "    print(f\"Peak VRAM: {peak_mem:.2f} GB\")\n",
                "    \n",
                "    del model, trainer, dataset\n",
                "    clean_memory()\n",
                "    \n",
                "    return results"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Run benchmarks\n",
                "print(\"=\"*60)\n",
                "print(\"  ZENITH PERFORMANCE BENCHMARK - PROPER BACKEND\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Run PyTorch baseline first\n",
                "pytorch_results = run_benchmark(use_zenith=False, steps=50)\n",
                "\n",
                "# Run Zenith optimized\n",
                "zenith_results = run_benchmark(use_zenith=True, steps=50)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Calculate and display results\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"  BENCHMARK RESULTS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "speedup = (pytorch_results['total_time'] - zenith_results['total_time']) / pytorch_results['total_time'] * 100\n",
                "\n",
                "print(f\"\\nPyTorch Baseline: {pytorch_results['total_time']:.2f}s\")\n",
                "print(f\"Zenith Optimized: {zenith_results['total_time']:.2f}s\")\n",
                "print(f\"\\nSpeedup: {speedup:+.2f}%\")\n",
                "print(f\"\\nPeak VRAM (PyTorch): {pytorch_results['peak_vram_gb']:.2f} GB\")\n",
                "print(f\"Peak VRAM (Zenith):  {zenith_results['peak_vram_gb']:.2f} GB\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Expected Results\n",
                "\n",
                "With the **proper** Zenith backend (not pass-through):\n",
                "- Training speedup: +5-15%\n",
                "- Same or lower VRAM usage\n",
                "- Numerical accuracy preserved (MSE ~ 0)"
            ],
            "metadata": {}
        }
    ]
}