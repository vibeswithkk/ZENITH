{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Zenith Transformer End-to-End Benchmark\n",
                "\n",
                "Tests complete BERT-style transformer inference:\n",
                "- Self-Attention (Q, K, V projections + attention scores)\n",
                "- Feed-Forward Network (Linear + GELU + Linear)\n",
                "- LayerNorm + Residual connections\n",
                "\n",
                "**GPU**: NVIDIA T4"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Cell 1: Setup\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.version.cuda}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cell 2: Define BERT-style Transformer Block\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, hidden_size, num_heads, intermediate_size, dropout=0.0):\n",
                "        super().__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.num_heads = num_heads\n",
                "        self.head_dim = hidden_size // num_heads\n",
                "        \n",
                "        # Self-Attention\n",
                "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
                "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
                "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
                "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
                "        \n",
                "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
                "        \n",
                "        # Feed-Forward\n",
                "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
                "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
                "        self.ffn_norm = nn.LayerNorm(hidden_size)\n",
                "        \n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        batch, seq_len, _ = x.shape\n",
                "        \n",
                "        # Self-Attention\n",
                "        residual = x\n",
                "        x = self.attn_norm(x)\n",
                "        \n",
                "        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
                "        k = self.k_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
                "        v = self.v_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
                "        \n",
                "        # Scaled dot-product attention\n",
                "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
                "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
                "        attn_output = torch.matmul(attn_weights, v)\n",
                "        \n",
                "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch, seq_len, self.hidden_size)\n",
                "        attn_output = self.out_proj(attn_output)\n",
                "        x = residual + self.dropout(attn_output)\n",
                "        \n",
                "        # Feed-Forward\n",
                "        residual = x\n",
                "        x = self.ffn_norm(x)\n",
                "        x = self.fc1(x)\n",
                "        x = torch.nn.functional.gelu(x)\n",
                "        x = self.fc2(x)\n",
                "        x = residual + self.dropout(x)\n",
                "        \n",
                "        return x\n",
                "\n",
                "print(\"TransformerBlock defined.\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cell 3: Define benchmark function\n",
                "def benchmark_model(model, input_tensor, warmup=10, runs=50):\n",
                "    \"\"\"Benchmark model inference.\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        # Warmup\n",
                "        for _ in range(warmup):\n",
                "            _ = model(input_tensor)\n",
                "        \n",
                "        torch.cuda.synchronize()\n",
                "        times = []\n",
                "        \n",
                "        for _ in range(runs):\n",
                "            torch.cuda.synchronize()\n",
                "            start = time.perf_counter()\n",
                "            _ = model(input_tensor)\n",
                "            torch.cuda.synchronize()\n",
                "            times.append((time.perf_counter() - start) * 1000)\n",
                "    \n",
                "    return {\n",
                "        'mean_ms': np.mean(times),\n",
                "        'std_ms': np.std(times),\n",
                "        'min_ms': np.min(times),\n",
                "        'throughput': 1000 / np.mean(times),\n",
                "    }\n",
                "\n",
                "print(\"Benchmark function ready.\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cell 4: BERT-Base Configuration Benchmark\n",
                "print(\"=\" * 60)\n",
                "print(\"BERT-BASE CONFIGURATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "config_base = {\n",
                "    'hidden_size': 768,\n",
                "    'num_heads': 12,\n",
                "    'intermediate_size': 3072,\n",
                "    'num_layers': 12,\n",
                "}\n",
                "\n",
                "batch_sizes = [1, 8, 32]\n",
                "seq_lengths = [128, 512]\n",
                "\n",
                "results_base = []\n",
                "\n",
                "for batch in batch_sizes:\n",
                "    for seq_len in seq_lengths:\n",
                "        # FP32\n",
                "        model_fp32 = TransformerBlock(\n",
                "            config_base['hidden_size'],\n",
                "            config_base['num_heads'],\n",
                "            config_base['intermediate_size']\n",
                "        ).cuda().float()\n",
                "        \n",
                "        x_fp32 = torch.randn(batch, seq_len, config_base['hidden_size'], device='cuda', dtype=torch.float32)\n",
                "        result_fp32 = benchmark_model(model_fp32, x_fp32)\n",
                "        \n",
                "        # FP16\n",
                "        model_fp16 = TransformerBlock(\n",
                "            config_base['hidden_size'],\n",
                "            config_base['num_heads'],\n",
                "            config_base['intermediate_size']\n",
                "        ).cuda().half()\n",
                "        \n",
                "        x_fp16 = torch.randn(batch, seq_len, config_base['hidden_size'], device='cuda', dtype=torch.float16)\n",
                "        result_fp16 = benchmark_model(model_fp16, x_fp16)\n",
                "        \n",
                "        speedup = result_fp32['mean_ms'] / result_fp16['mean_ms']\n",
                "        \n",
                "        print(f\"Batch={batch}, Seq={seq_len}:\")\n",
                "        print(f\"  FP32: {result_fp32['mean_ms']:.2f} ms ({result_fp32['throughput']:.1f} samples/s)\")\n",
                "        print(f\"  FP16: {result_fp16['mean_ms']:.2f} ms ({result_fp16['throughput']:.1f} samples/s)\")\n",
                "        print(f\"  Speedup: {speedup:.2f}x\")\n",
                "        \n",
                "        results_base.append({\n",
                "            'batch': batch,\n",
                "            'seq_len': seq_len,\n",
                "            'fp32_ms': result_fp32['mean_ms'],\n",
                "            'fp16_ms': result_fp16['mean_ms'],\n",
                "            'speedup': speedup,\n",
                "        })\n",
                "        \n",
                "        del model_fp32, model_fp16, x_fp32, x_fp16\n",
                "        torch.cuda.empty_cache()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cell 5: Full 12-Layer BERT Test\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"FULL 12-LAYER BERT-BASE\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "class BERTEncoder(nn.Module):\n",
                "    def __init__(self, hidden_size, num_heads, intermediate_size, num_layers):\n",
                "        super().__init__()\n",
                "        self.layers = nn.ModuleList([\n",
                "            TransformerBlock(hidden_size, num_heads, intermediate_size)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "    \n",
                "    def forward(self, x):\n",
                "        for layer in self.layers:\n",
                "            x = layer(x)\n",
                "        return x\n",
                "\n",
                "# Test with batch=8, seq=128 (common inference scenario)\n",
                "batch, seq_len = 8, 128\n",
                "\n",
                "# FP32 Full Model\n",
                "bert_fp32 = BERTEncoder(\n",
                "    config_base['hidden_size'],\n",
                "    config_base['num_heads'],\n",
                "    config_base['intermediate_size'],\n",
                "    config_base['num_layers']\n",
                ").cuda().float()\n",
                "\n",
                "x_fp32 = torch.randn(batch, seq_len, config_base['hidden_size'], device='cuda', dtype=torch.float32)\n",
                "result_fp32 = benchmark_model(bert_fp32, x_fp32, warmup=5, runs=20)\n",
                "\n",
                "# FP16 Full Model\n",
                "bert_fp16 = BERTEncoder(\n",
                "    config_base['hidden_size'],\n",
                "    config_base['num_heads'],\n",
                "    config_base['intermediate_size'],\n",
                "    config_base['num_layers']\n",
                ").cuda().half()\n",
                "\n",
                "x_fp16 = torch.randn(batch, seq_len, config_base['hidden_size'], device='cuda', dtype=torch.float16)\n",
                "result_fp16 = benchmark_model(bert_fp16, x_fp16, warmup=5, runs=20)\n",
                "\n",
                "speedup = result_fp32['mean_ms'] / result_fp16['mean_ms']\n",
                "\n",
                "print(f\"Configuration: Batch={batch}, Seq={seq_len}, Layers=12\")\n",
                "print(f\"FP32: {result_fp32['mean_ms']:.2f} ms\")\n",
                "print(f\"FP16: {result_fp16['mean_ms']:.2f} ms\")\n",
                "print(f\"Speedup: {speedup:.2f}x\")\n",
                "\n",
                "# Memory usage\n",
                "torch.cuda.reset_peak_memory_stats()\n",
                "_ = bert_fp32(x_fp32)\n",
                "mem_fp32 = torch.cuda.max_memory_allocated() / 1e6\n",
                "\n",
                "torch.cuda.reset_peak_memory_stats()\n",
                "_ = bert_fp16(x_fp16)\n",
                "mem_fp16 = torch.cuda.max_memory_allocated() / 1e6\n",
                "\n",
                "print(f\"\\nMemory (peak):\")\n",
                "print(f\"FP32: {mem_fp32:.1f} MB\")\n",
                "print(f\"FP16: {mem_fp16:.1f} MB\")\n",
                "print(f\"Savings: {(1 - mem_fp16/mem_fp32)*100:.1f}%\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cell 6: Summary\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ZENITH TRANSFORMER BENCHMARK SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(\"\\nSingle Layer Results:\")\n",
                "print(f\"{'Batch':<8} {'Seq':<8} {'FP32 (ms)':<12} {'FP16 (ms)':<12} {'Speedup':<10}\")\n",
                "print(\"-\" * 50)\n",
                "for r in results_base:\n",
                "    print(f\"{r['batch']:<8} {r['seq_len']:<8} {r['fp32_ms']:<12.2f} {r['fp16_ms']:<12.2f} {r['speedup']:<10.2f}x\")\n",
                "\n",
                "print(f\"\\nFull 12-Layer BERT-Base (batch=8, seq=128):\")\n",
                "print(f\"  FP32: {result_fp32['mean_ms']:.2f} ms\")\n",
                "print(f\"  FP16: {result_fp16['mean_ms']:.2f} ms (Tensor Cores)\")\n",
                "print(f\"  Speedup: {speedup:.2f}x\")\n",
                "print(f\"  Memory Savings: {(1 - mem_fp16/mem_fp32)*100:.1f}%\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"This benchmark validates Zenith's optimization targets:\")\n",
                "print(\"- FP16 Tensor Core utilization\")\n",
                "print(\"- Memory efficiency improvement\")\n",
                "print(\"- End-to-end transformer acceleration\")\n",
                "print(\"=\" * 60)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}