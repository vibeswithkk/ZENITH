{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Zenith JAX Core Integration - Phase 1 Validation\n",
                "\n",
                "**Date:** 2025-12-28  \n",
                "**Purpose:** Validate Phase 1 implementation with real JAX on GPU\n",
                "\n",
                "## Components Being Tested:\n",
                "1. **Gradient Checkpointing** - `zenith.jax.checkpointing`\n",
                "2. **Memory Management** - `zenith.jax.memory_manager`\n",
                "3. **Mixed Precision Training** - `zenith.jax.mixed_precision`\n",
                "\n",
                "## Hardware Requirements:\n",
                "- GPU Runtime (T4 or better)\n",
                "- JAX with GPU support\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setup: Install Dependencies"
            ],
            "metadata": {
                "id": "setup_header"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Force reinstall pyzenith from GitHub (latest with JAX Core Integration)\n",
                "%pip uninstall pyzenith -y 2>/dev/null\n",
                "%pip install --no-cache-dir git+https://github.com/vibeswithkk/ZENITH.git --quiet\n",
                "\n",
                "# Verify JAX GPU support (pre-installed in Colab)\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "\n",
                "print(f\"JAX version: {jax.__version__}\")\n",
                "print(f\"Devices: {jax.devices()}\")\n",
                "print(f\"Default backend: {jax.default_backend()}\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Verify Zenith installation\n",
                "import zenith\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "\n",
                "# Import Phase 1 modules\n",
                "from zenith.jax.checkpointing import (\n",
                "    OptimalCheckpointSelector,\n",
                "    CheckpointPolicy,\n",
                "    checkpoint,\n",
                ")\n",
                "from zenith.jax.memory_manager import (\n",
                "    JAXActivationStore,\n",
                "    EvictionPolicy,\n",
                "    compute_array_size,\n",
                "    get_device_string,\n",
                ")\n",
                "from zenith.jax.mixed_precision import (\n",
                "    MixedPrecisionPolicy,\n",
                "    DynamicLossScaler,\n",
                "    LossScalerConfig,\n",
                "    ZenithMixedPrecision,\n",
                "    create_policy,\n",
                "    detect_best_precision,\n",
                ")\n",
                "\n",
                "print(\"\\nAll Phase 1 modules imported successfully!\")"
            ],
            "metadata": {
                "id": "import_zenith"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 1: Gradient Checkpointing Validation"
            ],
            "metadata": {
                "id": "section1_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.1: OptimalCheckpointSelector Algorithms\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.1: OptimalCheckpointSelector Algorithms\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for num_layers in [4, 12, 24, 48]:\n",
                "    selector = OptimalCheckpointSelector(num_layers=num_layers)\n",
                "    sqrt_ckpts = selector.select_sqrt()\n",
                "    dp_ckpts = selector.select_dp()\n",
                "    reduction = selector.estimate_memory_reduction(sqrt_ckpts)\n",
                "\n",
                "    print(f\"Layers: {num_layers} -> sqrt: {len(sqrt_ckpts)}, DP: {len(dp_ckpts)}, reduction: {reduction:.1f}%\")\n",
                "\n",
                "print(\"\\n[PASS] OptimalCheckpointSelector works correctly!\")"
            ],
            "metadata": {
                "id": "test_1_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.2: JAX Checkpoint with Gradients\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.2: JAX Checkpoint with Gradients\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def mlp_layer(x, w1, w2):\n",
                "    h = jnp.dot(x, w1)\n",
                "    h = jax.nn.relu(h)\n",
                "    return jnp.dot(h, w2)\n",
                "\n",
                "# Use jax.checkpoint directly\n",
                "checkpointed_mlp = jax.checkpoint(mlp_layer)\n",
                "\n",
                "key = jax.random.PRNGKey(42)\n",
                "x = jax.random.normal(key, (32, 64))\n",
                "w1 = jax.random.normal(key, (64, 128))\n",
                "w2 = jax.random.normal(key, (128, 64))\n",
                "\n",
                "def loss_fn(x, w1, w2):\n",
                "    out = checkpointed_mlp(x, w1, w2)\n",
                "    return jnp.mean(out ** 2)\n",
                "\n",
                "grads = jax.grad(loss_fn, argnums=(1, 2))(x, w1, w2)\n",
                "\n",
                "print(f\"W1 grad shape: {grads[0].shape}, norm: {jnp.linalg.norm(grads[0]):.4f}\")\n",
                "print(f\"W2 grad shape: {grads[1].shape}, norm: {jnp.linalg.norm(grads[1]):.4f}\")\n",
                "\n",
                "assert jnp.all(jnp.isfinite(grads[0])) and jnp.all(jnp.isfinite(grads[1]))\n",
                "print(\"\\n[PASS] JAX checkpoint with gradients works!\")"
            ],
            "metadata": {
                "id": "test_1_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.3: Zenith checkpoint() wrapper\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.3: Zenith checkpoint() wrapper\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def simple_fn(x, w):\n",
                "    return jax.nn.relu(jnp.dot(x, w))\n",
                "\n",
                "# Use Zenith's checkpoint wrapper\n",
                "ckpt_fn = checkpoint(simple_fn, policy=CheckpointPolicy.DOTS_SAVEABLE)\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "x = jax.random.normal(key, (8, 16))\n",
                "w = jax.random.normal(key, (16, 16))\n",
                "\n",
                "# Forward pass\n",
                "out = ckpt_fn(x, w)\n",
                "print(f\"Forward output shape: {out.shape}\")\n",
                "\n",
                "# Gradient computation\n",
                "def loss(w):\n",
                "    return jnp.mean(ckpt_fn(x, w) ** 2)\n",
                "\n",
                "grad_w = jax.grad(loss)(w)\n",
                "print(f\"Gradient shape: {grad_w.shape}, norm: {jnp.linalg.norm(grad_w):.4f}\")\n",
                "\n",
                "assert jnp.all(jnp.isfinite(grad_w)) and jnp.linalg.norm(grad_w) > 0\n",
                "print(\"\\n[PASS] Zenith checkpoint() wrapper works!\")"
            ],
            "metadata": {
                "id": "test_1_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 2: Memory Management Validation"
            ],
            "metadata": {
                "id": "section2_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.1: JAXActivationStore with JAX Arrays\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.1: JAXActivationStore with JAX Arrays\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "store = JAXActivationStore(max_memory_bytes=100 * 1024 * 1024)\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "arrays = {}\n",
                "\n",
                "for i in range(5):\n",
                "    arr = jax.random.normal(key, (1024, 1024))\n",
                "    arrays[i] = arr\n",
                "    success = store.store(layer_id=i, array=arr)\n",
                "    size_mb = compute_array_size(arr) / 1024 / 1024\n",
                "    print(f\"Stored layer {i}: {size_mb:.2f} MB, success={success}\")\n",
                "\n",
                "print(f\"\\nTotal memory: {store.memory_usage / 1024 / 1024:.2f} MB\")\n",
                "\n",
                "# Retrieve and verify\n",
                "for i in range(5):\n",
                "    retrieved = store.retrieve(layer_id=i)\n",
                "    if retrieved is not None:\n",
                "        match = jnp.allclose(retrieved, arrays[i])\n",
                "        assert match, f\"Layer {i} mismatch!\"\n",
                "\n",
                "print(\"\\n[PASS] JAXActivationStore works with JAX arrays!\")"
            ],
            "metadata": {
                "id": "test_2_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.2: Eviction Under Memory Pressure\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.2: Eviction Under Memory Pressure\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Budget = 5 MB, but we store 10 arrays of ~1MB each\n",
                "# This FORCES eviction to happen\n",
                "store = JAXActivationStore(\n",
                "    max_memory_bytes=5 * 1024 * 1024,  # 5 MB budget\n",
                "    eviction_policy=EvictionPolicy.LRU,\n",
                ")\n",
                "\n",
                "key = jax.random.PRNGKey(42)\n",
                "\n",
                "# Store 10 arrays of ~1MB each = 10 MB total, but budget is only 5 MB\n",
                "for i in range(10):\n",
                "    arr = jax.random.normal(key, (512, 512))  # 512*512*4 = 1MB\n",
                "    store.store(layer_id=i, array=arr)\n",
                "\n",
                "stats = store.statistics\n",
                "print(f\"Stored: {stats['store_count']} arrays\")\n",
                "print(f\"Evicted: {stats['eviction_count']} arrays\")\n",
                "print(f\"Current memory: {stats['current_memory_mb']:.2f} MB\")\n",
                "print(f\"Currently in store: {stats['stored_count']} arrays\")\n",
                "\n",
                "# With 5MB budget and 1MB arrays, we can only keep 5 arrays\n",
                "# So 5 should have been evicted\n",
                "assert stats['eviction_count'] > 0, \"Eviction should have occurred!\"\n",
                "assert stats['current_memory_bytes'] <= 5 * 1024 * 1024, \"Memory exceeds budget!\"\n",
                "\n",
                "print(\"\\n[PASS] Eviction works correctly under memory pressure!\")"
            ],
            "metadata": {
                "id": "test_2_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.3: Device Detection\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.3: Device Detection\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "gpu_array = jax.random.normal(key, (100, 100))\n",
                "\n",
                "device_str = get_device_string(gpu_array)\n",
                "print(f\"Array device: {device_str}\")\n",
                "\n",
                "print(\"\\n[PASS] Device detection completed!\")"
            ],
            "metadata": {
                "id": "test_2_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 3: Mixed Precision Validation"
            ],
            "metadata": {
                "id": "section3_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.1: MixedPrecisionPolicy\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.1: MixedPrecisionPolicy Dtype Conversions\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for mode in ['fp32', 'bf16', 'fp16']:\n",
                "    policy = create_policy(mode)\n",
                "    print(f\"{mode.upper()}: param={policy.param_dtype}, compute={policy.compute_dtype}, scaling={policy.requires_loss_scaling}\")\n",
                "\n",
                "# Test BF16 conversion\n",
                "policy_bf16 = MixedPrecisionPolicy.bf16()\n",
                "arr = jnp.ones((10, 10), dtype=jnp.float32)\n",
                "arr_bf16 = arr.astype(jnp.bfloat16)\n",
                "\n",
                "print(f\"\\nOriginal: {arr.dtype} -> BF16: {arr_bf16.dtype}\")\n",
                "assert arr_bf16.dtype == jnp.bfloat16\n",
                "\n",
                "print(\"\\n[PASS] MixedPrecisionPolicy works!\")"
            ],
            "metadata": {
                "id": "test_3_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.2: DynamicLossScaler\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.2: DynamicLossScaler\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "scaler = DynamicLossScaler(LossScalerConfig(\n",
                "    initial_scale=2**15,\n",
                "    growth_factor=2.0,\n",
                "    backoff_factor=0.5,\n",
                "    growth_interval=5,\n",
                "))\n",
                "\n",
                "print(f\"Initial scale: {scaler.scale}\")\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "params = jax.random.normal(key, (64, 64))\n",
                "x = jax.random.normal(key, (32, 64))\n",
                "\n",
                "def loss_fn(p, x):\n",
                "    return jnp.mean((jnp.dot(x, p)) ** 2)\n",
                "\n",
                "for step in range(10):\n",
                "    def scaled_loss(p):\n",
                "        return scaler.scale_loss(loss_fn(p, x))\n",
                "    \n",
                "    grads = jax.grad(scaled_loss)(params)\n",
                "    unscaled, is_finite = scaler.unscale_grads({'p': grads})\n",
                "    scaler.update(is_finite)\n",
                "    \n",
                "    if step % 3 == 0:\n",
                "        print(f\"Step {step}: scale={scaler.scale:.0f}, finite={is_finite}\")\n",
                "\n",
                "print(\"\\n[PASS] DynamicLossScaler works!\")"
            ],
            "metadata": {
                "id": "test_3_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.3: ZenithMixedPrecision End-to-End\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.3: ZenithMixedPrecision End-to-End\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "mp = ZenithMixedPrecision(policy='bf16')\n",
                "\n",
                "print(f\"Policy: {mp.policy.mode.value}\")\n",
                "print(f\"Compute dtype: {mp.policy.compute_dtype}\")\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "params = {\n",
                "    'w1': jax.random.normal(key, (64, 128), dtype=jnp.float32),\n",
                "    'w2': jax.random.normal(key, (128, 64), dtype=jnp.float32),\n",
                "}\n",
                "\n",
                "print(f\"\\nOriginal: w1={params['w1'].dtype}, w2={params['w2'].dtype}\")\n",
                "\n",
                "# Cast to BF16\n",
                "compute_params = mp.cast_to_compute(params)\n",
                "print(f\"Compute: w1={compute_params['w1'].dtype}, w2={compute_params['w2'].dtype}\")\n",
                "\n",
                "assert compute_params['w1'].dtype == jnp.bfloat16\n",
                "\n",
                "# Cast back\n",
                "back = mp.cast_to_param(compute_params)\n",
                "print(f\"Back: w1={back['w1'].dtype}\")\n",
                "\n",
                "assert back['w1'].dtype == jnp.float32\n",
                "\n",
                "print(\"\\n[PASS] ZenithMixedPrecision works!\")"
            ],
            "metadata": {
                "id": "test_3_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.4: Hardware Detection\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.4: Hardware Detection\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "best = detect_best_precision()\n",
                "print(f\"Detected best precision: {best}\")\n",
                "\n",
                "for device in jax.devices():\n",
                "    print(f\"  Device: {device.platform} - {device}\")\n",
                "\n",
                "print(\"\\n[PASS] Hardware detection works!\")"
            ],
            "metadata": {
                "id": "test_3_4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 4: Performance Benchmark"
            ],
            "metadata": {
                "id": "section4_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# BENCHMARK: Mixed Precision Speedup\n",
                "print(\"=\"*60)\n",
                "print(\"BENCHMARK: Mixed Precision Speedup\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import time\n",
                "\n",
                "SIZE = 2048\n",
                "key = jax.random.PRNGKey(0)\n",
                "\n",
                "a_fp32 = jax.random.normal(key, (SIZE, SIZE), dtype=jnp.float32)\n",
                "b_fp32 = jax.random.normal(key, (SIZE, SIZE), dtype=jnp.float32)\n",
                "a_bf16 = a_fp32.astype(jnp.bfloat16)\n",
                "b_bf16 = b_fp32.astype(jnp.bfloat16)\n",
                "\n",
                "matmul = jax.jit(jnp.dot)\n",
                "\n",
                "# Warmup\n",
                "_ = matmul(a_fp32, b_fp32).block_until_ready()\n",
                "_ = matmul(a_bf16, b_bf16).block_until_ready()\n",
                "\n",
                "N = 20\n",
                "\n",
                "start = time.time()\n",
                "for _ in range(N):\n",
                "    matmul(a_fp32, b_fp32).block_until_ready()\n",
                "t_fp32 = (time.time() - start) / N * 1000\n",
                "\n",
                "start = time.time()\n",
                "for _ in range(N):\n",
                "    matmul(a_bf16, b_bf16).block_until_ready()\n",
                "t_bf16 = (time.time() - start) / N * 1000\n",
                "\n",
                "print(f\"Matrix: {SIZE}x{SIZE}\")\n",
                "print(f\"FP32: {t_fp32:.2f} ms\")\n",
                "print(f\"BF16: {t_bf16:.2f} ms (speedup: {t_fp32/t_bf16:.2f}x)\")\n",
                "\n",
                "print(\"\\n[BENCHMARK COMPLETE]\")"
            ],
            "metadata": {
                "id": "benchmark"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Final Summary"
            ],
            "metadata": {
                "id": "summary_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"=\"*70)\n",
                "print(\"ZENITH JAX PHASE 1 VALIDATION - COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\"\"\n",
                "All Tests Passed:\n",
                "\n",
                " Section 1: Gradient Checkpointing\n",
                "   [PASS] TEST 1.1: OptimalCheckpointSelector\n",
                "   [PASS] TEST 1.2: JAX checkpoint with gradients\n",
                "   [PASS] TEST 1.3: Zenith checkpoint() wrapper\n",
                "\n",
                " Section 2: Memory Management\n",
                "   [PASS] TEST 2.1: JAXActivationStore\n",
                "   [PASS] TEST 2.2: Eviction under pressure\n",
                "   [PASS] TEST 2.3: Device detection\n",
                "\n",
                " Section 3: Mixed Precision\n",
                "   [PASS] TEST 3.1: MixedPrecisionPolicy\n",
                "   [PASS] TEST 3.2: DynamicLossScaler\n",
                "   [PASS] TEST 3.3: ZenithMixedPrecision\n",
                "   [PASS] TEST 3.4: Hardware detection\n",
                "\n",
                " Section 4: Performance\n",
                "   [DONE] Mixed precision benchmark\n",
                "\n",
                "Phase 1 JAX Core Integration VALIDATED!\n",
                "Ready for Phase 2: XLA Backend & ONNX Export\n",
                "\"\"\")\n",
                "print(\"=\"*70)"
            ],
            "metadata": {
                "id": "summary"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}