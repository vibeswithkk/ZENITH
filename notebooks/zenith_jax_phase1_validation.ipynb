{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Zenith JAX Core Integration - Phase 1 Validation\n",
                "\n",
                "**Date:** 2025-12-28  \n",
                "**Purpose:** Validate Phase 1 implementation with real JAX on GPU\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# STEP 1: Clean install from GitHub\n",
                "# This ensures we get the LATEST version with all fixes\n",
                "!pip uninstall pyzenith -y 2>/dev/null\n",
                "!pip cache purge 2>/dev/null\n",
                "!pip install --force-reinstall --no-cache-dir git+https://github.com/vibeswithkk/ZENITH.git -q\n",
                "\n",
                "# Verify installation\n",
                "import zenith\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "\n",
                "# Check JAX\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "print(f\"JAX version: {jax.__version__}\")\n",
                "print(f\"Devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# STEP 2: Import Phase 1 modules\n",
                "from zenith.jax.checkpointing import (\n",
                "    OptimalCheckpointSelector,\n",
                "    CheckpointPolicy,\n",
                "    checkpoint,\n",
                ")\n",
                "from zenith.jax.memory_manager import (\n",
                "    JAXActivationStore,\n",
                "    EvictionPolicy,\n",
                "    compute_array_size,\n",
                "    get_device_string,\n",
                ")\n",
                "from zenith.jax.mixed_precision import (\n",
                "    MixedPrecisionPolicy,\n",
                "    DynamicLossScaler,\n",
                "    LossScalerConfig,\n",
                "    ZenithMixedPrecision,\n",
                "    create_policy,\n",
                "    detect_best_precision,\n",
                ")\n",
                "\n",
                "print(\"All Phase 1 modules imported!\")"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 1: Gradient Checkpointing"
            ],
            "metadata": {
                "id": "section1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.1: OptimalCheckpointSelector\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.1: OptimalCheckpointSelector\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for n in [4, 12, 24, 48]:\n",
                "    sel = OptimalCheckpointSelector(num_layers=n)\n",
                "    sqrt = sel.select_sqrt()\n",
                "    dp = sel.select_dp()\n",
                "    red = sel.estimate_memory_reduction(sqrt)\n",
                "    print(f\"Layers={n}: sqrt={len(sqrt)}, DP={len(dp)}, reduction={red:.1f}%\")\n",
                "\n",
                "print(\"\\n[PASS] TEST 1.1\")"
            ],
            "metadata": {
                "id": "test_1_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.2: JAX Checkpoint with Gradients\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.2: JAX Checkpoint with Gradients\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def mlp(x, w1, w2):\n",
                "    return jnp.dot(jax.nn.relu(jnp.dot(x, w1)), w2)\n",
                "\n",
                "ckpt_mlp = jax.checkpoint(mlp)\n",
                "\n",
                "key = jax.random.PRNGKey(42)\n",
                "x = jax.random.normal(key, (32, 64))\n",
                "w1 = jax.random.normal(key, (64, 128))\n",
                "w2 = jax.random.normal(key, (128, 64))\n",
                "\n",
                "grads = jax.grad(lambda x,w1,w2: jnp.mean(ckpt_mlp(x,w1,w2)**2), argnums=(1,2))(x,w1,w2)\n",
                "print(f\"Grad shapes: {grads[0].shape}, {grads[1].shape}\")\n",
                "assert jnp.all(jnp.isfinite(grads[0])) and jnp.all(jnp.isfinite(grads[1]))\n",
                "\n",
                "print(\"\\n[PASS] TEST 1.2\")"
            ],
            "metadata": {
                "id": "test_1_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.3: Zenith checkpoint() wrapper\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.3: Zenith checkpoint() wrapper\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def fn(x, w):\n",
                "    return jax.nn.relu(jnp.dot(x, w))\n",
                "\n",
                "ckpt_fn = checkpoint(fn, policy=CheckpointPolicy.DOTS_SAVEABLE)\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "x = jax.random.normal(key, (8, 16))\n",
                "w = jax.random.normal(key, (16, 16))\n",
                "\n",
                "out = ckpt_fn(x, w)\n",
                "grad_w = jax.grad(lambda w: jnp.mean(ckpt_fn(x, w)**2))(w)\n",
                "\n",
                "print(f\"Output: {out.shape}, Grad: {grad_w.shape}\")\n",
                "assert jnp.all(jnp.isfinite(grad_w))\n",
                "\n",
                "print(\"\\n[PASS] TEST 1.3\")"
            ],
            "metadata": {
                "id": "test_1_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 2: Memory Management"
            ],
            "metadata": {
                "id": "section2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.1: JAXActivationStore\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.1: JAXActivationStore\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "store = JAXActivationStore(max_memory_bytes=100*1024*1024)\n",
                "key = jax.random.PRNGKey(0)\n",
                "arrays = {}\n",
                "\n",
                "for i in range(5):\n",
                "    arr = jax.random.normal(key, (1024, 1024))\n",
                "    arrays[i] = arr\n",
                "    store.store(layer_id=i, array=arr)\n",
                "    print(f\"Stored layer {i}: {compute_array_size(arr)/1024/1024:.2f} MB\")\n",
                "\n",
                "for i in range(5):\n",
                "    ret = store.retrieve(layer_id=i)\n",
                "    assert ret is not None and jnp.allclose(ret, arrays[i])\n",
                "\n",
                "print(\"\\n[PASS] TEST 2.1\")"
            ],
            "metadata": {
                "id": "test_2_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.2: Eviction Under Memory Pressure\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.2: Eviction Under Memory Pressure\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 5 MB budget, 10 x 1MB arrays = must evict\n",
                "store = JAXActivationStore(max_memory_bytes=5*1024*1024, eviction_policy=EvictionPolicy.LRU)\n",
                "key = jax.random.PRNGKey(42)\n",
                "\n",
                "for i in range(10):\n",
                "    arr = jax.random.normal(key, (512, 512))  # 1MB each\n",
                "    store.store(layer_id=i, array=arr)\n",
                "\n",
                "stats = store.statistics\n",
                "print(f\"Stored: {stats['store_count']}, Evicted: {stats['eviction_count']}\")\n",
                "print(f\"Current: {stats['current_memory_mb']:.2f} MB, In store: {stats['stored_count']}\")\n",
                "\n",
                "assert stats['eviction_count'] > 0, \"Eviction should have occurred!\"\n",
                "\n",
                "print(\"\\n[PASS] TEST 2.2\")"
            ],
            "metadata": {
                "id": "test_2_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.3: Device Detection\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.3: Device Detection\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "arr = jax.random.normal(jax.random.PRNGKey(0), (100, 100))\n",
                "dev = get_device_string(arr)\n",
                "print(f\"Array device: {dev}\")\n",
                "\n",
                "print(\"\\n[PASS] TEST 2.3\")"
            ],
            "metadata": {
                "id": "test_2_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 3: Mixed Precision"
            ],
            "metadata": {
                "id": "section3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.1: MixedPrecisionPolicy\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.1: MixedPrecisionPolicy\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for mode in ['fp32', 'bf16', 'fp16']:\n",
                "    p = create_policy(mode)\n",
                "    print(f\"{mode}: compute={p.compute_dtype}, scaling={p.requires_loss_scaling}\")\n",
                "\n",
                "arr = jnp.ones((10, 10), dtype=jnp.float32)\n",
                "arr_bf16 = arr.astype(jnp.bfloat16)\n",
                "assert arr_bf16.dtype == jnp.bfloat16\n",
                "\n",
                "print(\"\\n[PASS] TEST 3.1\")"
            ],
            "metadata": {
                "id": "test_3_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.2: DynamicLossScaler\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.2: DynamicLossScaler\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "scaler = DynamicLossScaler(LossScalerConfig(\n",
                "    initial_scale=2**15,\n",
                "    growth_factor=2.0,\n",
                "    backoff_factor=0.5,\n",
                "    growth_interval=5,\n",
                "))\n",
                "\n",
                "print(f\"Initial scale: {scaler.scale}\")\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "params = jax.random.normal(key, (64, 64))\n",
                "x = jax.random.normal(key, (32, 64))\n",
                "\n",
                "def loss_fn(p, x):\n",
                "    return jnp.mean((jnp.dot(x, p)) ** 2)\n",
                "\n",
                "for step in range(10):\n",
                "    def scaled_loss(p):\n",
                "        return scaler.scale_loss(loss_fn(p, x))\n",
                "    \n",
                "    grads = jax.grad(scaled_loss)(params)\n",
                "    unscaled, is_finite = scaler.unscale_grads({'p': grads})\n",
                "    scaler.update(is_finite)\n",
                "    \n",
                "    if step % 3 == 0:\n",
                "        print(f\"Step {step}: scale={scaler.scale:.0f}, finite={is_finite}\")\n",
                "\n",
                "print(\"\\n[PASS] TEST 3.2\")"
            ],
            "metadata": {
                "id": "test_3_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.3: ZenithMixedPrecision\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.3: ZenithMixedPrecision\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "mp = ZenithMixedPrecision(policy='bf16')\n",
                "print(f\"Policy: {mp.policy.mode.value}\")\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "params = {\n",
                "    'w1': jax.random.normal(key, (64, 128), dtype=jnp.float32),\n",
                "    'w2': jax.random.normal(key, (128, 64), dtype=jnp.float32),\n",
                "}\n",
                "\n",
                "compute_params = mp.cast_to_compute(params)\n",
                "print(f\"Original: {params['w1'].dtype} -> Compute: {compute_params['w1'].dtype}\")\n",
                "assert compute_params['w1'].dtype == jnp.bfloat16\n",
                "\n",
                "back = mp.cast_to_param(compute_params)\n",
                "print(f\"Back: {back['w1'].dtype}\")\n",
                "assert back['w1'].dtype == jnp.float32\n",
                "\n",
                "print(\"\\n[PASS] TEST 3.3\")"
            ],
            "metadata": {
                "id": "test_3_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.4: Hardware Detection\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.4: Hardware Detection\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "best = detect_best_precision()\n",
                "print(f\"Best precision: {best}\")\n",
                "for d in jax.devices():\n",
                "    print(f\"  Device: {d}\")\n",
                "\n",
                "print(\"\\n[PASS] TEST 3.4\")"
            ],
            "metadata": {
                "id": "test_3_4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 4: Benchmark"
            ],
            "metadata": {
                "id": "section4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# BENCHMARK: Mixed Precision Speedup\n",
                "print(\"=\"*60)\n",
                "print(\"BENCHMARK: Mixed Precision Speedup\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import time\n",
                "\n",
                "SIZE = 2048\n",
                "key = jax.random.PRNGKey(0)\n",
                "a32 = jax.random.normal(key, (SIZE, SIZE), dtype=jnp.float32)\n",
                "b32 = jax.random.normal(key, (SIZE, SIZE), dtype=jnp.float32)\n",
                "a16 = a32.astype(jnp.bfloat16)\n",
                "b16 = b32.astype(jnp.bfloat16)\n",
                "\n",
                "matmul = jax.jit(jnp.dot)\n",
                "\n",
                "# Warmup\n",
                "matmul(a32, b32).block_until_ready()\n",
                "matmul(a16, b16).block_until_ready()\n",
                "\n",
                "N = 20\n",
                "t = time.time()\n",
                "for _ in range(N): matmul(a32, b32).block_until_ready()\n",
                "t32 = (time.time() - t) / N * 1000\n",
                "\n",
                "t = time.time()\n",
                "for _ in range(N): matmul(a16, b16).block_until_ready()\n",
                "t16 = (time.time() - t) / N * 1000\n",
                "\n",
                "print(f\"FP32: {t32:.2f} ms\")\n",
                "print(f\"BF16: {t16:.2f} ms (speedup: {t32/t16:.2f}x)\")\n",
                "\n",
                "print(\"\\n[BENCHMARK COMPLETE]\")"
            ],
            "metadata": {
                "id": "benchmark"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# FINAL SUMMARY\n",
                "print(\"=\"*70)\n",
                "print(\"ZENITH JAX PHASE 1 VALIDATION - ALL TESTS PASSED!\")\n",
                "print(\"=\"*70)\n",
                "print(\"\"\"\n",
                " Section 1: Gradient Checkpointing\n",
                "   [PASS] TEST 1.1: OptimalCheckpointSelector\n",
                "   [PASS] TEST 1.2: JAX checkpoint with gradients\n",
                "   [PASS] TEST 1.3: Zenith checkpoint() wrapper\n",
                "\n",
                " Section 2: Memory Management\n",
                "   [PASS] TEST 2.1: JAXActivationStore\n",
                "   [PASS] TEST 2.2: Eviction under pressure\n",
                "   [PASS] TEST 2.3: Device detection\n",
                "\n",
                " Section 3: Mixed Precision\n",
                "   [PASS] TEST 3.1: MixedPrecisionPolicy\n",
                "   [PASS] TEST 3.2: DynamicLossScaler\n",
                "   [PASS] TEST 3.3: ZenithMixedPrecision\n",
                "   [PASS] TEST 3.4: Hardware detection\n",
                "\n",
                " Section 4: Performance\n",
                "   [DONE] Mixed precision benchmark\n",
                "\n",
                "Phase 1 JAX Core Integration VALIDATED!\n",
                "Ready for Phase 2: XLA Backend & ONNX Export\n",
                "\"\"\")"
            ],
            "metadata": {
                "id": "summary"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}