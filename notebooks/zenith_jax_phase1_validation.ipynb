{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Zenith JAX Core Integration - Phase 1 Validation\n",
                "\n",
                "**Date:** 2025-12-28  \n",
                "**Purpose:** Validate Phase 1 implementation with real JAX on GPU\n",
                "\n",
                "## Components Being Tested:\n",
                "1. **Gradient Checkpointing** - `zenith.jax.checkpointing`\n",
                "2. **Memory Management** - `zenith.jax.memory_manager`\n",
                "3. **Mixed Precision Training** - `zenith.jax.mixed_precision`\n",
                "\n",
                "## Hardware Requirements:\n",
                "- GPU Runtime (T4 or better)\n",
                "- JAX with GPU support\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setup: Install Dependencies"
            ],
            "metadata": {
                "id": "setup_header"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install pyzenith from PyPI\n",
                "%pip install pyzenith --quiet\n",
                "\n",
                "# Verify JAX GPU support (pre-installed in Colab)\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "\n",
                "print(f\"JAX version: {jax.__version__}\")\n",
                "print(f\"Devices: {jax.devices()}\")\n",
                "print(f\"Default backend: {jax.default_backend()}\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Verify Zenith installation\n",
                "import zenith\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "\n",
                "# Import Phase 1 modules\n",
                "from zenith.jax.checkpointing import (\n",
                "    OptimalCheckpointSelector,\n",
                "    ZenithCheckpointer,\n",
                "    CheckpointConfig,\n",
                "    CheckpointPolicy,\n",
                "    SelectionMethod,\n",
                "    checkpoint,\n",
                "    checkpoint_sequential,\n",
                "    remat,\n",
                ")\n",
                "from zenith.jax.memory_manager import (\n",
                "    JAXActivationStore,\n",
                "    JAXMemoryManager,\n",
                "    JAXMemoryConfig,\n",
                "    EvictionPolicy,\n",
                "    compute_array_size,\n",
                "    get_device_string,\n",
                ")\n",
                "from zenith.jax.mixed_precision import (\n",
                "    MixedPrecisionPolicy,\n",
                "    DynamicLossScaler,\n",
                "    LossScalerConfig,\n",
                "    ZenithMixedPrecision,\n",
                "    PrecisionMode,\n",
                "    create_policy,\n",
                "    detect_best_precision,\n",
                ")\n",
                "\n",
                "print(\"\\nAll Phase 1 modules imported successfully!\")"
            ],
            "metadata": {
                "id": "import_zenith"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 1: Gradient Checkpointing Validation\n",
                "\n",
                "We will test:\n",
                "1. `OptimalCheckpointSelector` algorithms (sqrt, DP, uniform)\n",
                "2. `checkpoint()` function with actual `jax.grad`\n",
                "3. Memory reduction measurement"
            ],
            "metadata": {
                "id": "section1_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.1: OptimalCheckpointSelector Algorithms\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.1: OptimalCheckpointSelector Algorithms\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Test with various network sizes\n",
                "for num_layers in [4, 12, 24, 48, 96]:\n",
                "    selector = OptimalCheckpointSelector(num_layers=num_layers)\n",
                "\n",
                "    sqrt_ckpts = selector.select_sqrt()\n",
                "    dp_ckpts = selector.select_dp()\n",
                "    uniform_ckpts = selector.select_uniform(num_checkpoints=4)\n",
                "\n",
                "    reduction = selector.estimate_memory_reduction(sqrt_ckpts)\n",
                "\n",
                "    print(f\"\\nLayers: {num_layers}\")\n",
                "    print(f\"  sqrt: {len(sqrt_ckpts)} checkpoints -> {sqrt_ckpts[:5]}{'...' if len(sqrt_ckpts) > 5 else ''}\")\n",
                "    print(f\"  DP:   {len(dp_ckpts)} checkpoints -> {dp_ckpts[:5]}{'...' if len(dp_ckpts) > 5 else ''}\")\n",
                "    print(f\"  Memory reduction (sqrt): {reduction:.1f}%\")\n",
                "\n",
                "print(\"\\n[PASS] OptimalCheckpointSelector works correctly!\")"
            ],
            "metadata": {
                "id": "test_1_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.2: checkpoint() with Real JAX Gradient Computation\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.2: checkpoint() with Real JAX Gradients\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "\n",
                "# Define a simple MLP layer\n",
                "def mlp_layer(x, w1, w2):\n",
                "    \"\"\"Simple MLP: x -> linear -> relu -> linear\"\"\"\n",
                "    h = jnp.dot(x, w1)\n",
                "    h = jax.nn.relu(h)\n",
                "    return jnp.dot(h, w2)\n",
                "\n",
                "# Create checkpointed version using Zenith\n",
                "checkpointed_mlp = checkpoint(mlp_layer, policy=CheckpointPolicy.DOTS_SAVEABLE)\n",
                "\n",
                "# Initialize weights\n",
                "key = jax.random.PRNGKey(42)\n",
                "x = jax.random.normal(key, (32, 64))  # batch=32, features=64\n",
                "w1 = jax.random.normal(key, (64, 128))\n",
                "w2 = jax.random.normal(key, (128, 64))\n",
                "\n",
                "# Loss function\n",
                "def loss_fn(x, w1, w2):\n",
                "    out = checkpointed_mlp(x, w1, w2)\n",
                "    return jnp.mean(out ** 2)\n",
                "\n",
                "# Compute gradients\n",
                "grads = jax.grad(loss_fn, argnums=(1, 2))(x, w1, w2)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"W1 shape: {w1.shape}, W1 grad shape: {grads[0].shape}\")\n",
                "print(f\"W2 shape: {w2.shape}, W2 grad shape: {grads[1].shape}\")\n",
                "print(f\"Grad W1 norm: {jnp.linalg.norm(grads[0]):.4f}\")\n",
                "print(f\"Grad W2 norm: {jnp.linalg.norm(grads[1]):.4f}\")\n",
                "\n",
                "# Verify gradients are not zero or NaN\n",
                "assert jnp.all(jnp.isfinite(grads[0])), \"W1 gradients contain inf/nan!\"\n",
                "assert jnp.all(jnp.isfinite(grads[1])), \"W2 gradients contain inf/nan!\"\n",
                "assert jnp.linalg.norm(grads[0]) > 0, \"W1 gradients are zero!\"\n",
                "assert jnp.linalg.norm(grads[1]) > 0, \"W2 gradients are zero!\"\n",
                "\n",
                "print(\"\\n[PASS] checkpoint() produces valid gradients!\")"
            ],
            "metadata": {
                "id": "test_1_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.3: checkpoint_sequential() with Multiple Layers\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.3: checkpoint_sequential() with Multiple Layers\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create a sequence of layers\n",
                "key = jax.random.PRNGKey(0)\n",
                "keys = jax.random.split(key, 6)\n",
                "\n",
                "# Initialize 6 layers' weights\n",
                "layers_weights = [\n",
                "    jax.random.normal(keys[i], (64, 64)) for i in range(6)\n",
                "]\n",
                "\n",
                "# Define layer functions\n",
                "def make_layer_fn(w):\n",
                "    def layer(x):\n",
                "        return jax.nn.relu(jnp.dot(x, w))\n",
                "    return layer\n",
                "\n",
                "layer_fns = [make_layer_fn(w) for w in layers_weights]\n",
                "\n",
                "# Input\n",
                "x = jax.random.normal(key, (16, 64))\n",
                "\n",
                "# Run checkpoint_sequential\n",
                "output = checkpoint_sequential(\n",
                "    functions=layer_fns,\n",
                "    input_value=x,\n",
                "    segments=3,  # Use 3 segments\n",
                "    policy=CheckpointPolicy.DOTS_SAVEABLE,\n",
                ")\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Output mean: {jnp.mean(output):.4f}\")\n",
                "print(f\"Output std: {jnp.std(output):.4f}\")\n",
                "\n",
                "# Verify output is valid\n",
                "assert output.shape == x.shape, f\"Shape mismatch: {output.shape} vs {x.shape}\"\n",
                "assert jnp.all(jnp.isfinite(output)), \"Output contains inf/nan!\"\n",
                "\n",
                "print(\"\\n[PASS] checkpoint_sequential() works correctly!\")"
            ],
            "metadata": {
                "id": "test_1_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 1.4: Gradient Correctness - Compare with/without checkpointing\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 1.4: Gradient Correctness Verification\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def transformer_block(x, w_attn, w_ff):\n",
                "    \"\"\"Simplified transformer block\"\"\"\n",
                "    # Self-attention (simplified)\n",
                "    attn = jnp.dot(x, w_attn)\n",
                "    attn = jax.nn.softmax(attn, axis=-1)\n",
                "    h = jnp.dot(attn, x)\n",
                "\n",
                "    # FFN\n",
                "    out = jnp.dot(h, w_ff)\n",
                "    out = jax.nn.gelu(out)\n",
                "\n",
                "    return out + x  # Residual\n",
                "\n",
                "# Create checkpointed version\n",
                "checkpointed_block = checkpoint(transformer_block, policy=CheckpointPolicy.NOTHING)\n",
                "\n",
                "# Initialize\n",
                "key = jax.random.PRNGKey(123)\n",
                "x = jax.random.normal(key, (8, 32, 64))  # (batch, seq, dim)\n",
                "w_attn = jax.random.normal(key, (64, 64))\n",
                "w_ff = jax.random.normal(key, (64, 64))\n",
                "\n",
                "# Loss functions\n",
                "def loss_no_ckpt(x, w_attn, w_ff):\n",
                "    out = transformer_block(x, w_attn, w_ff)\n",
                "    return jnp.mean(out ** 2)\n",
                "\n",
                "def loss_with_ckpt(x, w_attn, w_ff):\n",
                "    out = checkpointed_block(x, w_attn, w_ff)\n",
                "    return jnp.mean(out ** 2)\n",
                "\n",
                "# Compute gradients both ways\n",
                "grads_no_ckpt = jax.grad(loss_no_ckpt, argnums=(1, 2))(x, w_attn, w_ff)\n",
                "grads_with_ckpt = jax.grad(loss_with_ckpt, argnums=(1, 2))(x, w_attn, w_ff)\n",
                "\n",
                "# Compare gradients\n",
                "diff_attn = jnp.abs(grads_no_ckpt[0] - grads_with_ckpt[0]).max()\n",
                "diff_ff = jnp.abs(grads_no_ckpt[1] - grads_with_ckpt[1]).max()\n",
                "\n",
                "print(f\"Gradient difference (w_attn): {diff_attn:.2e}\")\n",
                "print(f\"Gradient difference (w_ff): {diff_ff:.2e}\")\n",
                "\n",
                "# Tolerance for floating-point differences (checkpointing may have small numerical differences)\n",
                "TOLERANCE = 1e-4\n",
                "assert diff_attn < TOLERANCE, f\"w_attn gradient difference too large: {diff_attn}\"\n",
                "assert diff_ff < TOLERANCE, f\"w_ff gradient difference too large: {diff_ff}\"\n",
                "\n",
                "print(f\"\\n[PASS] Gradients match within tolerance {TOLERANCE}!\")"
            ],
            "metadata": {
                "id": "test_1_4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 2: Memory Management Validation\n",
                "\n",
                "We will test:\n",
                "1. `JAXActivationStore` with real JAX arrays\n",
                "2. Memory tracking accuracy\n",
                "3. Eviction policies\n",
                "4. CPU offloading"
            ],
            "metadata": {
                "id": "section2_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.1: JAXActivationStore with JAX Arrays\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.1: JAXActivationStore with JAX Arrays\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "store = JAXActivationStore(max_memory_bytes=100 * 1024 * 1024)  # 100 MB\n",
                "\n",
                "# Store JAX arrays\n",
                "key = jax.random.PRNGKey(0)\n",
                "arrays = {}\n",
                "\n",
                "for i in range(5):\n",
                "    arr = jax.random.normal(key, (1024, 1024))  # ~4MB each\n",
                "    arrays[i] = arr\n",
                "    success = store.store(layer_id=i, array=arr)\n",
                "    print(f\"Stored layer {i}: {compute_array_size(arr) / 1024 / 1024:.2f} MB, success={success}\")\n",
                "\n",
                "print(f\"\\nTotal memory usage: {store.memory_usage / 1024 / 1024:.2f} MB\")\n",
                "print(f\"Stored arrays: {len(store)}\")\n",
                "\n",
                "# Retrieve and verify\n",
                "for i in range(5):\n",
                "    retrieved = store.retrieve(layer_id=i)\n",
                "    if retrieved is not None:\n",
                "        match = jnp.allclose(retrieved, arrays[i])\n",
                "        print(f\"Layer {i} retrieved, matches original: {match}\")\n",
                "        assert match, f\"Layer {i} data mismatch!\"\n",
                "\n",
                "print(\"\\n[PASS] JAXActivationStore works with JAX arrays!\")"
            ],
            "metadata": {
                "id": "test_2_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.2: Eviction Under Memory Pressure\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.2: Eviction Under Memory Pressure\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Small memory budget to force eviction\n",
                "store = JAXActivationStore(\n",
                "    max_memory_bytes=10 * 1024 * 1024,  # 10 MB\n",
                "    eviction_policy=EvictionPolicy.LRU,\n",
                ")\n",
                "\n",
                "key = jax.random.PRNGKey(42)\n",
                "\n",
                "# Store more than budget allows\n",
                "for i in range(10):\n",
                "    arr = jax.random.normal(key, (512, 512))  # ~1MB each\n",
                "    store.store(layer_id=i, array=arr)\n",
                "\n",
                "stats = store.statistics\n",
                "print(f\"Stored: {stats['store_count']} arrays\")\n",
                "print(f\"Evicted: {stats['eviction_count']} arrays\")\n",
                "print(f\"Current memory: {stats['current_memory_mb']:.2f} MB\")\n",
                "print(f\"Peak memory: {stats['peak_memory_mb']:.2f} MB\")\n",
                "print(f\"Currently stored: {stats['stored_count']} arrays\")\n",
                "\n",
                "# Verify eviction happened\n",
                "assert stats['eviction_count'] > 0, \"Eviction should have occurred!\"\n",
                "assert stats['current_memory_bytes'] <= 10 * 1024 * 1024, \"Memory exceeds budget!\"\n",
                "\n",
                "print(\"\\n[PASS] Eviction works correctly under memory pressure!\")"
            ],
            "metadata": {
                "id": "test_2_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.3: Device Detection for JAX Arrays\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.3: Device Detection\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create arrays on different devices\n",
                "key = jax.random.PRNGKey(0)\n",
                "gpu_array = jax.random.normal(key, (100, 100))\n",
                "cpu_array = jax.device_put(gpu_array, jax.devices('cpu')[0])\n",
                "\n",
                "gpu_device = get_device_string(gpu_array)\n",
                "cpu_device = get_device_string(cpu_array)\n",
                "\n",
                "print(f\"GPU array device: {gpu_device}\")\n",
                "print(f\"CPU array device: {cpu_device}\")\n",
                "\n",
                "# Verify device detection\n",
                "assert 'gpu' in gpu_device.lower() or 'cuda' in gpu_device.lower() or 'tpu' in gpu_device.lower(), f\"Expected GPU device, got {gpu_device}\"\n",
                "assert 'cpu' in cpu_device.lower(), f\"Expected CPU device, got {cpu_device}\"\n",
                "\n",
                "print(\"\\n[PASS] Device detection works correctly!\")"
            ],
            "metadata": {
                "id": "test_2_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 2.4: JAXMemoryManager with CPU Offloading\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 2.4: JAXMemoryManager with CPU Offloading\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "config = JAXMemoryConfig(\n",
                "    max_memory_bytes=50 * 1024 * 1024,  # 50 MB\n",
                "    enable_offloading=True,\n",
                "    offload_threshold_bytes=5 * 1024 * 1024,  # 5 MB threshold\n",
                ")\n",
                "\n",
                "manager = JAXMemoryManager(config=config)\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "\n",
                "# Store a large array (should be offloaded)\n",
                "large_array = jax.random.normal(key, (2048, 2048))  # ~16 MB\n",
                "success = manager.store(layer_id=0, array=large_array, allow_offload=True)\n",
                "print(f\"Large array stored: {success}\")\n",
                "\n",
                "# Store a small array (should stay on GPU)\n",
                "small_array = jax.random.normal(key, (256, 256))  # ~0.25 MB\n",
                "success = manager.store(layer_id=1, array=small_array, allow_offload=True)\n",
                "print(f\"Small array stored: {success}\")\n",
                "\n",
                "stats = manager.get_statistics()\n",
                "print(f\"\\nOn-device memory: {stats['current_memory_mb']:.2f} MB\")\n",
                "print(f\"Offloaded arrays: {stats['offloaded_count']}\")\n",
                "print(f\"Offloaded size: {stats['offloaded_mb']:.2f} MB\")\n",
                "print(f\"Total managed: {stats['total_managed_mb']:.2f} MB\")\n",
                "\n",
                "# Retrieve and verify\n",
                "retrieved_large = manager.retrieve(layer_id=0, prefetch=True)\n",
                "retrieved_small = manager.retrieve(layer_id=1)\n",
                "\n",
                "print(f\"\\nLarge array retrieved shape: {retrieved_large.shape}\")\n",
                "print(f\"Small array retrieved shape: {retrieved_small.shape}\")\n",
                "\n",
                "# Verify data integrity\n",
                "assert jnp.allclose(retrieved_small, small_array), \"Small array data mismatch!\"\n",
                "# Note: large array may have been moved to CPU and back, slight numerical differences possible\n",
                "\n",
                "print(\"\\n[PASS] JAXMemoryManager works with CPU offloading!\")"
            ],
            "metadata": {
                "id": "test_2_4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 3: Mixed Precision Validation\n",
                "\n",
                "We will test:\n",
                "1. `MixedPrecisionPolicy` dtype conversions\n",
                "2. `DynamicLossScaler` with actual gradients\n",
                "3. `ZenithMixedPrecision` end-to-end training"
            ],
            "metadata": {
                "id": "section3_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.1: MixedPrecisionPolicy Dtype Conversions\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.1: MixedPrecisionPolicy Dtype Conversions\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Test FP32 policy\n",
                "policy_fp32 = MixedPrecisionPolicy.fp32()\n",
                "print(f\"FP32 policy:\")\n",
                "print(f\"  param_dtype: {policy_fp32.param_dtype}\")\n",
                "print(f\"  compute_dtype: {policy_fp32.compute_dtype}\")\n",
                "print(f\"  requires_loss_scaling: {policy_fp32.requires_loss_scaling}\")\n",
                "\n",
                "# Test BF16 policy\n",
                "policy_bf16 = MixedPrecisionPolicy.bf16()\n",
                "print(f\"\\nBF16 policy:\")\n",
                "print(f\"  param_dtype: {policy_bf16.param_dtype}\")\n",
                "print(f\"  compute_dtype: {policy_bf16.compute_dtype}\")\n",
                "print(f\"  requires_loss_scaling: {policy_bf16.requires_loss_scaling}\")\n",
                "\n",
                "# Test FP16 policy\n",
                "policy_fp16 = MixedPrecisionPolicy.fp16()\n",
                "print(f\"\\nFP16 policy:\")\n",
                "print(f\"  param_dtype: {policy_fp16.param_dtype}\")\n",
                "print(f\"  compute_dtype: {policy_fp16.compute_dtype}\")\n",
                "print(f\"  requires_loss_scaling: {policy_fp16.requires_loss_scaling}\")\n",
                "\n",
                "# Verify dtype conversions work\n",
                "arr = jnp.ones((10, 10), dtype=jnp.float32)\n",
                "bf16_dtype = policy_bf16.get_jax_dtype('bfloat16')\n",
                "arr_bf16 = arr.astype(bf16_dtype)\n",
                "\n",
                "print(f\"\\nOriginal dtype: {arr.dtype}\")\n",
                "print(f\"BF16 dtype: {arr_bf16.dtype}\")\n",
                "\n",
                "assert arr_bf16.dtype == jnp.bfloat16, \"BF16 conversion failed!\"\n",
                "\n",
                "print(\"\\n[PASS] MixedPrecisionPolicy dtype conversions work!\")"
            ],
            "metadata": {
                "id": "test_3_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.2: DynamicLossScaler with Real Gradients\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.2: DynamicLossScaler with Real Gradients\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "scaler = DynamicLossScaler(LossScalerConfig(\n",
                "    initial_scale=2**15,\n",
                "    growth_factor=2.0,\n",
                "    backoff_factor=0.5,\n",
                "    growth_interval=5,\n",
                "))\n",
                "\n",
                "print(f\"Initial scale: {scaler.scale}\")\n",
                "\n",
                "# Simulate training loop\n",
                "key = jax.random.PRNGKey(0)\n",
                "params = jax.random.normal(key, (64, 64))\n",
                "\n",
                "def loss_fn(params, x):\n",
                "    return jnp.mean((jnp.dot(x, params)) ** 2)\n",
                "\n",
                "x = jax.random.normal(key, (32, 64))\n",
                "\n",
                "# Training steps with scaling\n",
                "for step in range(10):\n",
                "    # Scale loss\n",
                "    loss = loss_fn(params, x)\n",
                "    scaled_loss = scaler.scale_loss(loss)\n",
                "\n",
                "    # Compute scaled gradients\n",
                "    def scaled_loss_fn(params):\n",
                "        return scaler.scale_loss(loss_fn(params, x))\n",
                "\n",
                "    scaled_grads = jax.grad(scaled_loss_fn)(params)\n",
                "\n",
                "    # Unscale and check\n",
                "    grads, is_finite = scaler.unscale_grads({'params': scaled_grads})\n",
                "\n",
                "    # Update scaler\n",
                "    scaler.update(is_finite)\n",
                "\n",
                "    if step % 2 == 0:\n",
                "        print(f\"Step {step}: loss={float(loss):.4f}, scale={scaler.scale:.0f}, finite={is_finite}\")\n",
                "\n",
                "print(f\"\\nFinal scale: {scaler.scale}\")\n",
                "print(f\"Total overflow count: {scaler.state.overflow_count}\")\n",
                "\n",
                "print(\"\\n[PASS] DynamicLossScaler works with real gradients!\")"
            ],
            "metadata": {
                "id": "test_3_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.3: ZenithMixedPrecision End-to-End\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.3: ZenithMixedPrecision End-to-End Training\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create mixed precision handler with BF16\n",
                "mp = ZenithMixedPrecision(policy='bf16')\n",
                "\n",
                "print(f\"Policy: {mp.policy.mode.value}\")\n",
                "print(f\"Compute dtype: {mp.policy.compute_dtype}\")\n",
                "print(f\"Has scaler: {mp.scaler is not None}\")\n",
                "\n",
                "# Initialize model params in FP32\n",
                "key = jax.random.PRNGKey(0)\n",
                "params = {\n",
                "    'w1': jax.random.normal(key, (64, 128), dtype=jnp.float32),\n",
                "    'w2': jax.random.normal(key, (128, 64), dtype=jnp.float32),\n",
                "}\n",
                "\n",
                "print(f\"\\nOriginal param dtypes:\")\n",
                "print(f\"  w1: {params['w1'].dtype}\")\n",
                "print(f\"  w2: {params['w2'].dtype}\")\n",
                "\n",
                "# Cast to compute dtype\n",
                "compute_params = mp.cast_to_compute(params)\n",
                "\n",
                "print(f\"\\nCompute param dtypes:\")\n",
                "print(f\"  w1: {compute_params['w1'].dtype}\")\n",
                "print(f\"  w2: {compute_params['w2'].dtype}\")\n",
                "\n",
                "# Verify BF16\n",
                "assert compute_params['w1'].dtype == jnp.bfloat16, \"w1 not BF16!\"\n",
                "assert compute_params['w2'].dtype == jnp.bfloat16, \"w2 not BF16!\"\n",
                "\n",
                "# Simulate forward pass\n",
                "x = jax.random.normal(key, (32, 64)).astype(jnp.bfloat16)\n",
                "\n",
                "def forward(params, x):\n",
                "    h = jnp.dot(x, params['w1'])\n",
                "    h = jax.nn.relu(h)\n",
                "    return jnp.dot(h, params['w2'])\n",
                "\n",
                "output = forward(compute_params, x)\n",
                "print(f\"\\nOutput dtype: {output.dtype}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "\n",
                "# Cast back to param dtype\n",
                "back_to_fp32 = mp.cast_to_param(compute_params)\n",
                "print(f\"\\nBack to FP32:\")\n",
                "print(f\"  w1: {back_to_fp32['w1'].dtype}\")\n",
                "print(f\"  w2: {back_to_fp32['w2'].dtype}\")\n",
                "\n",
                "assert back_to_fp32['w1'].dtype == jnp.float32, \"w1 not back to FP32!\"\n",
                "\n",
                "print(\"\\n[PASS] ZenithMixedPrecision end-to-end works correctly!\")"
            ],
            "metadata": {
                "id": "test_3_3"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.4: FP16 with Loss Scaling Full Loop\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.4: FP16 with Loss Scaling Full Training Loop\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create FP16 handler (requires loss scaling)\n",
                "mp_fp16 = ZenithMixedPrecision(policy='fp16')\n",
                "\n",
                "print(f\"Policy: {mp_fp16.policy.mode.value}\")\n",
                "print(f\"Has scaler: {mp_fp16.scaler is not None}\")\n",
                "print(f\"Initial scale: {mp_fp16.scaler.scale if mp_fp16.scaler else 'N/A'}\")\n",
                "\n",
                "# Initialize params\n",
                "key = jax.random.PRNGKey(42)\n",
                "params = {'w': jax.random.normal(key, (32, 32), dtype=jnp.float32)}\n",
                "x = jax.random.normal(key, (16, 32), dtype=jnp.float32)\n",
                "\n",
                "def loss_fn(params, x):\n",
                "    out = jnp.dot(x, params['w'])\n",
                "    return jnp.mean(out ** 2)\n",
                "\n",
                "print(\"\\nTraining steps:\")\n",
                "for step in range(5):\n",
                "    # Cast to FP16\n",
                "    compute_params = mp_fp16.cast_to_compute(params)\n",
                "    compute_x = x.astype(jnp.float16)\n",
                "\n",
                "    # Compute loss\n",
                "    loss = loss_fn(compute_params, compute_x)\n",
                "\n",
                "    # Scale loss\n",
                "    scaled_loss = mp_fp16.scale_loss(loss)\n",
                "\n",
                "    # Compute gradients\n",
                "    def scaled_loss_fn(params):\n",
                "        return mp_fp16.scale_loss(loss_fn(params, compute_x))\n",
                "\n",
                "    grads = jax.grad(scaled_loss_fn)(compute_params)\n",
                "\n",
                "    # Unscale and handle\n",
                "    grads, is_finite = mp_fp16.handle_grads(grads)\n",
                "\n",
                "    # Update scale\n",
                "    mp_fp16.update_scale(is_finite)\n",
                "\n",
                "    print(f\"  Step {step}: loss={float(loss):.4f}, scale={mp_fp16.scaler.scale:.0f}, finite={is_finite}\")\n",
                "\n",
                "print(f\"\\nFinal stats:\")\n",
                "print(f\"  Total steps: {mp_fp16.stats.total_steps}\")\n",
                "print(f\"  Overflow count: {mp_fp16.stats.overflow_count}\")\n",
                "\n",
                "print(\"\\n[PASS] FP16 with loss scaling works correctly!\")"
            ],
            "metadata": {
                "id": "test_3_4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TEST 3.5: Hardware Detection\n",
                "print(\"=\"*60)\n",
                "print(\"TEST 3.5: Hardware-Based Precision Detection\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "best_precision = detect_best_precision()\n",
                "print(f\"Detected best precision: {best_precision}\")\n",
                "\n",
                "# On GPU, should recommend bf16\n",
                "print(f\"\\nDevice info:\")\n",
                "for device in jax.devices():\n",
                "    print(f\"  {device.platform}: {device}\")\n",
                "\n",
                "print(\"\\n[PASS] Hardware detection completed!\")"
            ],
            "metadata": {
                "id": "test_3_5"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Section 4: Performance Benchmarks\n",
                "\n",
                "Measure actual performance improvements from:\n",
                "1. Gradient checkpointing memory reduction\n",
                "2. Mixed precision speedup"
            ],
            "metadata": {
                "id": "section4_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# BENCHMARK 1: Gradient Checkpointing Memory Impact\n",
                "print(\"=\"*60)\n",
                "print(\"BENCHMARK 1: Checkpointing Memory Impact\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import time\n",
                "\n",
                "# Deep network simulation\n",
                "NUM_LAYERS = 24\n",
                "HIDDEN_DIM = 512\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "key = jax.random.PRNGKey(0)\n",
                "keys = jax.random.split(key, NUM_LAYERS + 1)\n",
                "\n",
                "# Initialize weights for all layers\n",
                "weights = [jax.random.normal(keys[i], (HIDDEN_DIM, HIDDEN_DIM)) for i in range(NUM_LAYERS)]\n",
                "x = jax.random.normal(keys[NUM_LAYERS], (BATCH_SIZE, HIDDEN_DIM))\n",
                "\n",
                "def layer_fn(x, w):\n",
                "    return jax.nn.relu(jnp.dot(x, w))\n",
                "\n",
                "# Without checkpointing\n",
                "def forward_no_ckpt(x, weights):\n",
                "    for w in weights:\n",
                "        x = layer_fn(x, w)\n",
                "    return x\n",
                "\n",
                "def loss_no_ckpt(x, weights):\n",
                "    return jnp.mean(forward_no_ckpt(x, weights) ** 2)\n",
                "\n",
                "# With checkpointing\n",
                "checkpointed_layer = checkpoint(layer_fn, policy=CheckpointPolicy.NOTHING)\n",
                "\n",
                "def forward_with_ckpt(x, weights):\n",
                "    for w in weights:\n",
                "        x = checkpointed_layer(x, w)\n",
                "    return x\n",
                "\n",
                "def loss_with_ckpt(x, weights):\n",
                "    return jnp.mean(forward_with_ckpt(x, weights) ** 2)\n",
                "\n",
                "# Warm up JIT\n",
                "grad_fn_no_ckpt = jax.jit(jax.grad(loss_no_ckpt, argnums=1))\n",
                "grad_fn_with_ckpt = jax.jit(jax.grad(loss_with_ckpt, argnums=1))\n",
                "\n",
                "# Warm up\n",
                "_ = grad_fn_no_ckpt(x, weights)\n",
                "_ = grad_fn_with_ckpt(x, weights)\n",
                "\n",
                "# Benchmark\n",
                "N_RUNS = 10\n",
                "\n",
                "start = time.time()\n",
                "for _ in range(N_RUNS):\n",
                "    grads = grad_fn_no_ckpt(x, weights)\n",
                "    jax.block_until_ready(grads)\n",
                "time_no_ckpt = (time.time() - start) / N_RUNS * 1000\n",
                "\n",
                "start = time.time()\n",
                "for _ in range(N_RUNS):\n",
                "    grads = grad_fn_with_ckpt(x, weights)\n",
                "    jax.block_until_ready(grads)\n",
                "time_with_ckpt = (time.time() - start) / N_RUNS * 1000\n",
                "\n",
                "print(f\"Network: {NUM_LAYERS} layers, {HIDDEN_DIM} hidden dim, batch {BATCH_SIZE}\")\n",
                "print(f\"\\nTime per backward pass:\")\n",
                "print(f\"  Without checkpointing: {time_no_ckpt:.2f} ms\")\n",
                "print(f\"  With checkpointing: {time_with_ckpt:.2f} ms\")\n",
                "print(f\"  Overhead: {((time_with_ckpt / time_no_ckpt) - 1) * 100:.1f}%\")\n",
                "\n",
                "# Memory estimation\n",
                "selector = OptimalCheckpointSelector(NUM_LAYERS)\n",
                "checkpoints = selector.select_sqrt()\n",
                "memory_reduction = selector.estimate_memory_reduction(checkpoints)\n",
                "print(f\"\\nEstimated memory reduction: {memory_reduction:.1f}%\")\n",
                "\n",
                "print(\"\\n[BENCHMARK COMPLETE]\")"
            ],
            "metadata": {
                "id": "benchmark_1"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# BENCHMARK 2: Mixed Precision Speedup\n",
                "print(\"=\"*60)\n",
                "print(\"BENCHMARK 2: Mixed Precision Speedup\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import time\n",
                "\n",
                "# Matrix multiplication benchmark\n",
                "SIZE = 2048\n",
                "key = jax.random.PRNGKey(0)\n",
                "\n",
                "a_fp32 = jax.random.normal(key, (SIZE, SIZE), dtype=jnp.float32)\n",
                "b_fp32 = jax.random.normal(key, (SIZE, SIZE), dtype=jnp.float32)\n",
                "\n",
                "a_bf16 = a_fp32.astype(jnp.bfloat16)\n",
                "b_bf16 = b_fp32.astype(jnp.bfloat16)\n",
                "\n",
                "a_fp16 = a_fp32.astype(jnp.float16)\n",
                "b_fp16 = b_fp32.astype(jnp.float16)\n",
                "\n",
                "# JIT compile\n",
                "matmul = jax.jit(jnp.dot)\n",
                "\n",
                "# Warm up\n",
                "_ = matmul(a_fp32, b_fp32)\n",
                "_ = matmul(a_bf16, b_bf16)\n",
                "_ = matmul(a_fp16, b_fp16)\n",
                "\n",
                "N_RUNS = 20\n",
                "\n",
                "# FP32\n",
                "start = time.time()\n",
                "for _ in range(N_RUNS):\n",
                "    c = matmul(a_fp32, b_fp32)\n",
                "    jax.block_until_ready(c)\n",
                "time_fp32 = (time.time() - start) / N_RUNS * 1000\n",
                "\n",
                "# BF16\n",
                "start = time.time()\n",
                "for _ in range(N_RUNS):\n",
                "    c = matmul(a_bf16, b_bf16)\n",
                "    jax.block_until_ready(c)\n",
                "time_bf16 = (time.time() - start) / N_RUNS * 1000\n",
                "\n",
                "# FP16\n",
                "start = time.time()\n",
                "for _ in range(N_RUNS):\n",
                "    c = matmul(a_fp16, b_fp16)\n",
                "    jax.block_until_ready(c)\n",
                "time_fp16 = (time.time() - start) / N_RUNS * 1000\n",
                "\n",
                "print(f\"Matrix size: {SIZE}x{SIZE}\")\n",
                "print(f\"\\nTime per matmul:\")\n",
                "print(f\"  FP32: {time_fp32:.2f} ms\")\n",
                "print(f\"  BF16: {time_bf16:.2f} ms (speedup: {time_fp32/time_bf16:.2f}x)\")\n",
                "print(f\"  FP16: {time_fp16:.2f} ms (speedup: {time_fp32/time_fp16:.2f}x)\")\n",
                "\n",
                "print(\"\\n[BENCHMARK COMPLETE]\")"
            ],
            "metadata": {
                "id": "benchmark_2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Final Summary"
            ],
            "metadata": {
                "id": "summary_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# FINAL SUMMARY\n",
                "print(\"=\"*70)\n",
                "print(\"ZENITH JAX PHASE 1 VALIDATION - FINAL SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\"\"\n",
                "TESTS COMPLETED:\n",
                "================\n",
                "\n",
                "Section 1: Gradient Checkpointing\n",
                "  [x] TEST 1.1: OptimalCheckpointSelector algorithms\n",
                "  [x] TEST 1.2: checkpoint() with real JAX gradients\n",
                "  [x] TEST 1.3: checkpoint_sequential() with multiple layers\n",
                "  [x] TEST 1.4: Gradient correctness verification\n",
                "\n",
                "Section 2: Memory Management\n",
                "  [x] TEST 2.1: JAXActivationStore with JAX arrays\n",
                "  [x] TEST 2.2: Eviction under memory pressure\n",
                "  [x] TEST 2.3: Device detection\n",
                "  [x] TEST 2.4: JAXMemoryManager with CPU offloading\n",
                "\n",
                "Section 3: Mixed Precision\n",
                "  [x] TEST 3.1: MixedPrecisionPolicy dtype conversions\n",
                "  [x] TEST 3.2: DynamicLossScaler with real gradients\n",
                "  [x] TEST 3.3: ZenithMixedPrecision end-to-end\n",
                "  [x] TEST 3.4: FP16 with loss scaling full loop\n",
                "  [x] TEST 3.5: Hardware detection\n",
                "\n",
                "Section 4: Performance Benchmarks\n",
                "  [x] BENCHMARK 1: Checkpointing memory impact\n",
                "  [x] BENCHMARK 2: Mixed precision speedup\n",
                "\n",
                "CONCLUSION:\n",
                "===========\n",
                "If all tests above show [PASS], then Phase 1 JAX Core Integration\n",
                "is validated and ready for production use.\n",
                "\n",
                "Proceed to Phase 2: XLA Backend & ONNX Export Enhancement.\n",
                "\"\"\")\n",
                "\n",
                "print(\"=\"*70)"
            ],
            "metadata": {
                "id": "final_summary"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}