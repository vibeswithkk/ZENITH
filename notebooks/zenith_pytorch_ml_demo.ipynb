{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith x PyTorch: Machine Learning Demo\n",
                "\n",
                "## Zenith as a Complementary Framework, Not a Replacement\n",
                "\n",
                "This notebook demonstrates how **Zenith works alongside PyTorch** to enhance your ML workflow:\n",
                "\n",
                "1. **Train with PyTorch** - Use familiar PyTorch APIs for model definition and training\n",
                "2. **Optimize with Zenith** - Apply Zenith's torch.compile backend for faster inference\n",
                "3. **Export with Zenith** - Convert to ONNX for production deployment\n",
                "4. **Benchmark** - Compare performance before and after Zenith optimization\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install git+https://github.com/vibeswithkk/ZENITH.git -q\n",
                "!pip install onnx onnxruntime onnxscript -q\n",
                "\n",
                "print(\"Setup complete!\")\n",
                "print(\"IMPORTANT: Please restart runtime now (Runtime > Restart runtime)\")\n",
                "print(\"Then run cells starting from the next cell (cell 2)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "import time\n",
                "import numpy as np\n",
                "\n",
                "# Zenith imports\n",
                "import zenith\n",
                "import zenith.torch as ztorch\n",
                "from zenith.backends import is_cuda_available, get_device\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Define Model (Pure PyTorch)\n",
                "\n",
                "We use standard PyTorch to define our model - **no Zenith code here**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MNISTClassifier(nn.Module):\n",
                "    \"\"\"A simple CNN for MNIST classification - Pure PyTorch.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
                "        self.fc2 = nn.Linear(128, 10)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.dropout = nn.Dropout(0.25)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.pool(self.relu(self.conv1(x)))  # 28x28 -> 14x14\n",
                "        x = self.pool(self.relu(self.conv2(x)))  # 14x14 -> 7x7\n",
                "        x = x.view(-1, 64 * 7 * 7)\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Create model\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = MNISTClassifier().to(device)\n",
                "\n",
                "print(f\"Model created on: {device}\")\n",
                "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Load Data (Pure PyTorch)\n",
                "\n",
                "Standard PyTorch data loading."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.1307,), (0.3081,))\n",
                "])\n",
                "\n",
                "# Load MNIST\n",
                "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
                "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
                "\n",
                "print(f\"Training samples: {len(train_dataset):,}\")\n",
                "print(f\"Test samples: {len(test_dataset):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Train Model (Pure PyTorch)\n",
                "\n",
                "Training loop using standard PyTorch - **Zenith NOT involved in training**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion):\n",
                "    \"\"\"Standard PyTorch training loop.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch_idx, (data, target) in enumerate(loader):\n",
                "        data, target = data.to(device), target.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        output = model(data)\n",
                "        loss = criterion(output, target)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pred = output.argmax(dim=1)\n",
                "        correct += pred.eq(target).sum().item()\n",
                "        total += target.size(0)\n",
                "    \n",
                "    return total_loss / len(loader), 100. * correct / total\n",
                "\n",
                "\n",
                "def evaluate(model, loader, criterion):\n",
                "    \"\"\"Evaluate model accuracy.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for data, target in loader:\n",
                "            data, target = data.to(device), target.to(device)\n",
                "            output = model(data)\n",
                "            total_loss += criterion(output, target).item()\n",
                "            pred = output.argmax(dim=1)\n",
                "            correct += pred.eq(target).sum().item()\n",
                "            total += target.size(0)\n",
                "    \n",
                "    return total_loss / len(loader), 100. * correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "epochs = 3  # Quick demo\n",
                "\n",
                "print(\"Training with Pure PyTorch...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "train_start = time.time()\n",
                "\n",
                "for epoch in range(1, epochs + 1):\n",
                "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
                "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
                "    \n",
                "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
                "          f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.2f}%\")\n",
                "\n",
                "train_time = time.time() - train_start\n",
                "print(\"=\" * 50)\n",
                "print(f\"Training complete in {train_time:.2f}s\")\n",
                "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Benchmark: PyTorch Native Inference\n",
                "\n",
                "Measure baseline inference speed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_inference(model_fn, test_loader, num_batches=50, warmup=5):\n",
                "    \"\"\"Benchmark inference speed.\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Warmup\n",
                "    with torch.no_grad():\n",
                "        for i, (data, _) in enumerate(test_loader):\n",
                "            if i >= warmup:\n",
                "                break\n",
                "            data = data.to(device)\n",
                "            _ = model_fn(data)\n",
                "    \n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.synchronize()\n",
                "    \n",
                "    # Benchmark\n",
                "    times = []\n",
                "    with torch.no_grad():\n",
                "        for i, (data, _) in enumerate(test_loader):\n",
                "            if i >= num_batches:\n",
                "                break\n",
                "            data = data.to(device)\n",
                "            \n",
                "            if torch.cuda.is_available():\n",
                "                torch.cuda.synchronize()\n",
                "            start = time.perf_counter()\n",
                "            \n",
                "            _ = model_fn(data)\n",
                "            \n",
                "            if torch.cuda.is_available():\n",
                "                torch.cuda.synchronize()\n",
                "            end = time.perf_counter()\n",
                "            \n",
                "            times.append((end - start) * 1000)  # ms\n",
                "    \n",
                "    return np.mean(times), np.std(times), np.min(times), np.max(times)\n",
                "\n",
                "# Baseline: PyTorch native\n",
                "print(\"Benchmarking PyTorch Native Inference...\")\n",
                "native_mean, native_std, native_min, native_max = benchmark_inference(\n",
                "    lambda x: model(x), test_loader\n",
                ")\n",
                "print(f\"PyTorch Native: {native_mean:.3f} +/- {native_std:.3f} ms/batch\")\n",
                "print(f\"  Range: [{native_min:.3f}, {native_max:.3f}] ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Apply Zenith Optimization\n",
                "\n",
                "Now we use **Zenith's torch.compile backend** to optimize inference.\n",
                "\n",
                "**Key Point**: The model was trained with PyTorch. Zenith only optimizes the compiled version."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPLYING ZENITH OPTIMIZATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Method 1: Using torch.compile with Zenith backend\n",
                "if ztorch.has_torch_compile():\n",
                "    print(\"\\n[Method 1] torch.compile with Zenith backend...\")\n",
                "    \n",
                "    # Create Zenith backend\n",
                "    zenith_backend = ztorch.create_backend(\n",
                "        target=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
                "        precision=\"fp32\"\n",
                "    )\n",
                "    \n",
                "    # Compile model with Zenith\n",
                "    compiled_model = torch.compile(model, backend=zenith_backend)\n",
                "    print(\"Model compiled with Zenith backend\")\n",
                "    \n",
                "    # Verify accuracy is preserved\n",
                "    _, compiled_acc = evaluate(compiled_model, test_loader, criterion)\n",
                "    print(f\"Compiled Model Accuracy: {compiled_acc:.2f}%\")\n",
                "else:\n",
                "    print(\"torch.compile not available, using eager mode\")\n",
                "    compiled_model = model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Method 2: Using @ztorch.compile decorator\n",
                "print(\"\\n[Method 2] Using @ztorch.compile decorator...\")\n",
                "\n",
                "@ztorch.compile(target=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "def optimized_forward(x):\n",
                "    return model(x)\n",
                "\n",
                "# Test it\n",
                "sample = next(iter(test_loader))[0].to(device)\n",
                "output = optimized_forward(sample)\n",
                "print(f\"Decorated function output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Benchmark: Zenith-Optimized Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Benchmarking Zenith-Optimized Inference...\")\n",
                "zenith_mean, zenith_std, zenith_min, zenith_max = benchmark_inference(\n",
                "    lambda x: compiled_model(x), test_loader\n",
                ")\n",
                "print(f\"Zenith Optimized: {zenith_mean:.3f} +/- {zenith_std:.3f} ms/batch\")\n",
                "print(f\"  Range: [{zenith_min:.3f}, {zenith_max:.3f}] ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance comparison\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"PERFORMANCE COMPARISON\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nPyTorch Native:    {native_mean:.3f} ms/batch\")\n",
                "print(f\"Zenith Optimized:  {zenith_mean:.3f} ms/batch\")\n",
                "\n",
                "speedup = native_mean / zenith_mean if zenith_mean > 0 else 1.0\n",
                "improvement = (native_mean - zenith_mean) / native_mean * 100\n",
                "\n",
                "print(f\"\\nSpeedup: {speedup:.2f}x\")\n",
                "print(f\"Improvement: {improvement:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Export to ONNX\n",
                "\n",
                "Export the trained model to ONNX for production deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import io\n",
                "import onnx\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"ONNX EXPORT\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Sample input for export\n",
                "sample_input = torch.randn(1, 1, 28, 28)\n",
                "onnx_path = \"/tmp/mnist_classifier.onnx\"\n",
                "\n",
                "# Export using standard PyTorch ONNX export (most reliable)\n",
                "model_cpu = model.cpu()\n",
                "model_cpu.eval()\n",
                "\n",
                "torch.onnx.export(\n",
                "    model_cpu, \n",
                "    sample_input,\n",
                "    onnx_path,\n",
                "    input_names=['input'],\n",
                "    output_names=['output'],\n",
                "    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
                "    opset_version=14\n",
                ")\n",
                "\n",
                "# Get file size\n",
                "import os\n",
                "onnx_size = os.path.getsize(onnx_path)\n",
                "print(f\"ONNX model saved to: {onnx_path}\")\n",
                "print(f\"ONNX model size: {onnx_size:,} bytes\")\n",
                "\n",
                "# Validate\n",
                "onnx_model = onnx.load(onnx_path)\n",
                "onnx.checker.check_model(onnx_model)\n",
                "print(\"ONNX validation: PASSED\")\n",
                "\n",
                "# Move model back to device\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify ONNX model with ONNX Runtime\n",
                "print(\"\\nVerifying ONNX model with ONNX Runtime...\")\n",
                "\n",
                "import onnxruntime as ort\n",
                "\n",
                "# Create session\n",
                "sess = ort.InferenceSession(onnx_path)\n",
                "\n",
                "# Run inference\n",
                "test_input = np.random.randn(1, 1, 28, 28).astype(np.float32)\n",
                "output = sess.run(None, {sess.get_inputs()[0].name: test_input})\n",
                "\n",
                "print(f\"ONNX Runtime output shape: {output[0].shape}\")\n",
                "print(\"ONNX Runtime inference: PASSED\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Convert to Zenith GraphIR\n",
                "\n",
                "Zenith's internal representation for advanced optimization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.adapters import PyTorchAdapter\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"ZENITH GRAPHIR CONVERSION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "adapter = PyTorchAdapter()\n",
                "sample = torch.randn(1, 1, 28, 28)\n",
                "\n",
                "# Convert to GraphIR\n",
                "graph = adapter.from_model(model.cpu(), sample_input=sample)\n",
                "\n",
                "print(f\"Graph name: {graph.name}\")\n",
                "print(f\"Input tensors: {len(graph.inputs)}\")\n",
                "print(f\"Output tensors: {len(graph.outputs)}\")\n",
                "\n",
                "# Move model back\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. Summary: Zenith as PyTorch Companion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"ZENITH x PYTORCH: MACHINE LEARNING DEMO SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\"\"\n",
                "+------------------------------------------------------------------+\n",
                "|                     WORKFLOW DEMONSTRATION                       |\n",
                "+------------------------------------------------------------------+\n",
                "\n",
                "  1. MODEL DEFINITION:   Pure PyTorch (nn.Module)           [CHECK]\n",
                "  2. DATA LOADING:       Pure PyTorch (DataLoader)          [CHECK]\n",
                "  3. TRAINING:           Pure PyTorch (optimizer.step())    [CHECK]\n",
                "  4. OPTIMIZATION:       Zenith torch.compile backend       [CHECK]\n",
                "  5. ONNX EXPORT:        PyTorch + ONNX                     [CHECK]\n",
                "  6. GRAPHIR CONVERT:    Zenith PyTorchAdapter              [CHECK]\n",
                "\n",
                "+------------------------------------------------------------------+\n",
                "|                     KEY TAKEAWAYS                                |\n",
                "+------------------------------------------------------------------+\n",
                "\n",
                "  - Zenith does NOT replace PyTorch for training\n",
                "  - Zenith ENHANCES inference performance\n",
                "  - Zenith SIMPLIFIES production deployment (ONNX)\n",
                "  - Zenith INTEGRATES seamlessly with existing code\n",
                "  - Your PyTorch knowledge remains 100% applicable\n",
                "\n",
                "\"\"\")\n",
                "\n",
                "print(f\"Model Accuracy:     {test_acc:.2f}%\")\n",
                "print(f\"Native Inference:   {native_mean:.3f} ms/batch\")\n",
                "print(f\"Zenith Inference:   {zenith_mean:.3f} ms/batch\")\n",
                "print(f\"Performance Gain:   {speedup:.2f}x faster\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"Zenith: Your PyTorch Companion for Production ML\")\n",
                "print(\"=\" * 70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}