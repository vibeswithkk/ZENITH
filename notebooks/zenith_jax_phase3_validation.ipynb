{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith JAX Phase 3 Validation\n",
                "\n",
                "**Custom Primitives and XLA Kernels Testing**\n",
                "\n",
                "This notebook validates Phase 3 implementation:\n",
                "- Fused Attention Primitive\n",
                "- Fused LayerNorm Primitive\n",
                "- Fused GELU Primitive\n",
                "- Fused Softmax Primitive\n",
                "- XLA Custom Kernels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install JAX with GPU support\n",
                "!pip install -q jax jaxlib\n",
                "\n",
                "# Clone Zenith repository\n",
                "!rm -rf ZENITH\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git\n",
                "\n",
                "# Add to Python path\n",
                "import sys\n",
                "sys.path.insert(0, '/content/ZENITH')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify imports\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from jax import grad, jit, vmap\n",
                "\n",
                "print(f\"JAX version: {jax.__version__}\")\n",
                "print(f\"Available devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Test Fused Attention Primitive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.jax.primitives import fused_attention, list_primitives\n",
                "\n",
                "print(\"Registered primitives:\", list_primitives())\n",
                "\n",
                "# Create test inputs\n",
                "key = jax.random.PRNGKey(42)\n",
                "batch, heads, seq, dim = 2, 4, 32, 64\n",
                "\n",
                "q = jax.random.normal(key, (batch, heads, seq, dim))\n",
                "k = jax.random.normal(key, (batch, heads, seq, dim))\n",
                "v = jax.random.normal(key, (batch, heads, seq, dim))\n",
                "\n",
                "# Test basic execution\n",
                "print(\"\\n[TEST 1] Basic Fused Attention\")\n",
                "output = fused_attention(q, k, v)\n",
                "print(f\"  Input shape: {q.shape}\")\n",
                "print(f\"  Output shape: {output.shape}\")\n",
                "print(f\"  Output finite: {jnp.all(jnp.isfinite(output))}\")\n",
                "assert output.shape == q.shape\n",
                "assert jnp.all(jnp.isfinite(output))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test numerical correctness against reference\n",
                "print(\"[TEST 2] Numerical Correctness - Attention\")\n",
                "\n",
                "def reference_attention(q, k, v):\n",
                "    scale = 1.0 / jnp.sqrt(q.shape[-1])\n",
                "    attn_weights = jnp.einsum('bhqd,bhkd->bhqk', q, k) * scale\n",
                "    attn_weights = jax.nn.softmax(attn_weights, axis=-1)\n",
                "    return jnp.einsum('bhqk,bhkd->bhqd', attn_weights, v)\n",
                "\n",
                "reference = reference_attention(q, k, v)\n",
                "zenith_output = fused_attention(q, k, v)\n",
                "\n",
                "max_diff = jnp.max(jnp.abs(zenith_output - reference))\n",
                "print(f\"  Max difference: {max_diff}\")\n",
                "assert max_diff < 1e-5, f\"Numerical error too large: {max_diff}\"\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test gradient computation\n",
                "print(\"[TEST 3] Gradient Computation - Attention\")\n",
                "\n",
                "def loss_fn(q, k, v):\n",
                "    out = fused_attention(q, k, v)\n",
                "    return jnp.sum(out)\n",
                "\n",
                "grads = grad(loss_fn, argnums=(0, 1, 2))(q, k, v)\n",
                "dq, dk, dv = grads\n",
                "\n",
                "print(f\"  dQ shape: {dq.shape}\")\n",
                "print(f\"  dK shape: {dk.shape}\")\n",
                "print(f\"  dV shape: {dv.shape}\")\n",
                "assert dq.shape == q.shape\n",
                "assert dk.shape == k.shape\n",
                "assert dv.shape == v.shape\n",
                "assert jnp.all(jnp.isfinite(dq))\n",
                "assert jnp.all(jnp.isfinite(dk))\n",
                "assert jnp.all(jnp.isfinite(dv))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test JIT compilation\n",
                "print(\"[TEST 4] JIT Compilation - Attention\")\n",
                "\n",
                "jit_attention = jit(fused_attention)\n",
                "jit_output = jit_attention(q, k, v)\n",
                "\n",
                "assert jit_output.shape == q.shape\n",
                "assert jnp.all(jnp.isfinite(jit_output))\n",
                "max_diff = jnp.max(jnp.abs(jit_output - zenith_output))\n",
                "print(f\"  JIT vs non-JIT max diff: {max_diff}\")\n",
                "assert max_diff < 1e-5\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test Fused LayerNorm Primitive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.jax.primitives import fused_layernorm\n",
                "\n",
                "print(\"[TEST 5] Fused LayerNorm\")\n",
                "\n",
                "batch, seq, dim = 4, 128, 512\n",
                "x = jax.random.normal(key, (batch, seq, dim))\n",
                "weight = jnp.ones(dim)\n",
                "bias = jnp.zeros(dim)\n",
                "\n",
                "output = fused_layernorm(x, weight, bias)\n",
                "print(f\"  Input shape: {x.shape}\")\n",
                "print(f\"  Output shape: {output.shape}\")\n",
                "assert output.shape == x.shape\n",
                "assert jnp.all(jnp.isfinite(output))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test LayerNorm numerical correctness\n",
                "print(\"[TEST 6] LayerNorm Numerical Correctness\")\n",
                "\n",
                "def reference_layernorm(x, weight, bias, eps=1e-5):\n",
                "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
                "    var = jnp.var(x, axis=-1, keepdims=True)\n",
                "    x_norm = (x - mean) / jnp.sqrt(var + eps)\n",
                "    return x_norm * weight + bias\n",
                "\n",
                "reference = reference_layernorm(x, weight, bias)\n",
                "zenith_output = fused_layernorm(x, weight, bias)\n",
                "\n",
                "max_diff = jnp.max(jnp.abs(zenith_output - reference))\n",
                "print(f\"  Max difference: {max_diff}\")\n",
                "assert max_diff < 1e-5\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test LayerNorm gradient\n",
                "print(\"[TEST 7] LayerNorm Gradient\")\n",
                "\n",
                "def ln_loss(x, weight, bias):\n",
                "    return jnp.sum(fused_layernorm(x, weight, bias))\n",
                "\n",
                "grads = grad(ln_loss, argnums=(0, 1, 2))(x, weight, bias)\n",
                "dx, dw, db = grads\n",
                "\n",
                "assert dx.shape == x.shape\n",
                "assert dw.shape == weight.shape\n",
                "assert db.shape == bias.shape\n",
                "assert jnp.all(jnp.isfinite(dx))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test Fused GELU Primitive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.jax.primitives import fused_gelu\n",
                "import math\n",
                "\n",
                "print(\"[TEST 8] Fused GELU\")\n",
                "\n",
                "x = jax.random.normal(key, (8, 64, 256))\n",
                "output_approx = fused_gelu(x, approximate=True)\n",
                "output_exact = fused_gelu(x, approximate=False)\n",
                "\n",
                "print(f\"  Input shape: {x.shape}\")\n",
                "print(f\"  Output (approx) shape: {output_approx.shape}\")\n",
                "print(f\"  Output (exact) shape: {output_exact.shape}\")\n",
                "assert jnp.all(jnp.isfinite(output_approx))\n",
                "assert jnp.all(jnp.isfinite(output_exact))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test GELU numerical correctness\n",
                "print(\"[TEST 9] GELU Numerical Correctness\")\n",
                "\n",
                "def reference_gelu_approx(x):\n",
                "    coeff = math.sqrt(2.0 / math.pi)\n",
                "    return 0.5 * x * (1.0 + jnp.tanh(coeff * (x + 0.044715 * x**3)))\n",
                "\n",
                "reference = reference_gelu_approx(x)\n",
                "zenith_output = fused_gelu(x, approximate=True)\n",
                "\n",
                "max_diff = jnp.max(jnp.abs(zenith_output - reference))\n",
                "print(f\"  Max difference: {max_diff}\")\n",
                "assert max_diff < 1e-5\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test GELU gradient\n",
                "print(\"[TEST 10] GELU Gradient\")\n",
                "\n",
                "def gelu_loss(x):\n",
                "    return jnp.sum(fused_gelu(x))\n",
                "\n",
                "dx = grad(gelu_loss)(x)\n",
                "assert dx.shape == x.shape\n",
                "assert jnp.all(jnp.isfinite(dx))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Test Fused Softmax Primitive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.jax.primitives import fused_softmax\n",
                "\n",
                "print(\"[TEST 11] Fused Softmax\")\n",
                "\n",
                "x = jax.random.normal(key, (8, 16, 32))\n",
                "output = fused_softmax(x)\n",
                "\n",
                "print(f\"  Input shape: {x.shape}\")\n",
                "print(f\"  Output shape: {output.shape}\")\n",
                "assert output.shape == x.shape\n",
                "assert jnp.all(jnp.isfinite(output))\n",
                "assert jnp.all(output >= 0)\n",
                "assert jnp.all(output <= 1)\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test softmax sums to 1\n",
                "print(\"[TEST 12] Softmax Sums to 1\")\n",
                "\n",
                "sums = jnp.sum(output, axis=-1)\n",
                "max_deviation = jnp.max(jnp.abs(sums - 1.0))\n",
                "print(f\"  Max deviation from 1: {max_deviation}\")\n",
                "assert max_deviation < 1e-5\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test numerical stability with large values\n",
                "print(\"[TEST 13] Softmax Numerical Stability\")\n",
                "\n",
                "large_x = jnp.array([1000.0, 1001.0, 1002.0])\n",
                "stable_output = fused_softmax(large_x)\n",
                "\n",
                "print(f\"  Large input: {large_x}\")\n",
                "print(f\"  Output: {stable_output}\")\n",
                "assert jnp.all(jnp.isfinite(stable_output))\n",
                "assert jnp.abs(jnp.sum(stable_output) - 1.0) < 1e-5\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test softmax gradient\n",
                "print(\"[TEST 14] Softmax Gradient\")\n",
                "\n",
                "def softmax_loss(x):\n",
                "    return jnp.sum(fused_softmax(x))\n",
                "\n",
                "dx = grad(softmax_loss)(x)\n",
                "assert dx.shape == x.shape\n",
                "assert jnp.all(jnp.isfinite(dx))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test XLA Kernels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.runtime.xla_kernels import (\n",
                "    xla_fused_attention,\n",
                "    xla_fused_layernorm,\n",
                "    xla_fused_softmax,\n",
                "    list_kernels,\n",
                "    get_kernel_registry,\n",
                ")\n",
                "\n",
                "print(\"[TEST 15] XLA Kernels Registration\")\n",
                "print(f\"  Registered kernels: {list_kernels()}\")\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[TEST 16] XLA Fused Attention\")\n",
                "\n",
                "q = jax.random.normal(key, (2, 4, 32, 64))\n",
                "k = jax.random.normal(key, (2, 4, 32, 64))\n",
                "v = jax.random.normal(key, (2, 4, 32, 64))\n",
                "\n",
                "xla_output = xla_fused_attention(q, k, v)\n",
                "print(f\"  Output shape: {xla_output.shape}\")\n",
                "assert xla_output.shape == q.shape\n",
                "assert jnp.all(jnp.isfinite(xla_output))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[TEST 17] XLA Fused LayerNorm\")\n",
                "\n",
                "x = jax.random.normal(key, (4, 128, 512))\n",
                "weight = jnp.ones(512)\n",
                "bias = jnp.zeros(512)\n",
                "\n",
                "xla_output = xla_fused_layernorm(x, weight, bias)\n",
                "print(f\"  Output shape: {xla_output.shape}\")\n",
                "assert xla_output.shape == x.shape\n",
                "assert jnp.all(jnp.isfinite(xla_output))\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[TEST 18] XLA Fused Softmax\")\n",
                "\n",
                "x = jax.random.normal(key, (8, 16, 32))\n",
                "xla_output = xla_fused_softmax(x)\n",
                "\n",
                "print(f\"  Output shape: {xla_output.shape}\")\n",
                "assert xla_output.shape == x.shape\n",
                "assert jnp.all(jnp.isfinite(xla_output))\n",
                "sums = jnp.sum(xla_output, axis=-1)\n",
                "assert jnp.max(jnp.abs(sums - 1.0)) < 1e-5\n",
                "print(\"  [PASSED]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"PHASE 3 VALIDATION COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "print(\"\")\n",
                "print(\"All tests passed:\")\n",
                "print(\"  [OK] Fused Attention: basic, numerical, gradients, JIT\")\n",
                "print(\"  [OK] Fused LayerNorm: basic, numerical, gradients\")\n",
                "print(\"  [OK] Fused GELU: basic, numerical, gradients\")\n",
                "print(\"  [OK] Fused Softmax: basic, sums-to-1, stability, gradients\")\n",
                "print(\"  [OK] XLA Kernels: attention, layernorm, softmax\")\n",
                "print(\"\")\n",
                "print(\"Phase 3 is PRODUCTION-READY!\")\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}