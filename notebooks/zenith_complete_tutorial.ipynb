{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Complete Tutorial\n",
                "\n",
                "Tutorial komprehensif untuk Zenith ML Framework.\n",
                "\n",
                "**Apa yang akan dipelajari:**\n",
                "1. Instalasi dan Setup\n",
                "2. Zenith Core API\n",
                "3. Zenith + PyTorch (Hybrid)\n",
                "4. Zenith + TensorFlow (Hybrid)\n",
                "5. Zenith + JAX (Hybrid)\n",
                "6. Zenith Full Performance\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Instalasi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cek GPU\n",
                "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
                "\n",
                "# Install Zenith dengan PyTorch support\n",
                "!pip install pyzenith[pytorch] -q --upgrade\n",
                "\n",
                "# Verifikasi\n",
                "import zenith\n",
                "print(f\"Zenith Version: {zenith.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Zenith Core API\n",
                "\n",
                "Zenith menyediakan beberapa modul utama:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 Import Modules\n",
                "import zenith\n",
                "from zenith import backends\n",
                "from zenith.core import GraphIR, DataType\n",
                "from zenith.optimization import PassManager\n",
                "\n",
                "# 2.2 Check backends\n",
                "print(\"Available backends:\", backends.get_available_backends())\n",
                "print(\"CPU available:\", backends.is_cpu_available())\n",
                "print(\"CUDA available:\", backends.is_cuda_available())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.3 GraphIR - Representasi intermediate\n",
                "graph = GraphIR(name=\"example_graph\")\n",
                "print(f\"Graph: {graph.name}\")\n",
                "print(f\"Nodes: {graph.num_nodes}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.4 Optimization Passes\n",
                "from zenith.optimization import (\n",
                "    ConstantFoldingPass,\n",
                "    DeadCodeEliminationPass,\n",
                "    OperatorFusionPass\n",
                ")\n",
                "\n",
                "print(\"Available Optimization Passes:\")\n",
                "print(\"  - ConstantFoldingPass: Evaluasi konstanta pada compile-time\")\n",
                "print(\"  - DeadCodeEliminationPass: Hapus operasi yang tidak digunakan\")\n",
                "print(\"  - OperatorFusionPass: Gabungkan operasi untuk efisiensi\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.5 Quantization\n",
                "import numpy as np\n",
                "from zenith.optimization.quantization import (\n",
                "    quantize,\n",
                "    dequantize,\n",
                "    CalibrationMethod\n",
                ")\n",
                "\n",
                "# Data sample\n",
                "data = np.random.randn(1, 768).astype(np.float32)\n",
                "\n",
                "# Quantize to INT8\n",
                "quantized, scale, zero_point = quantize(\n",
                "    data,\n",
                "    num_bits=8,\n",
                "    method=CalibrationMethod.MINMAX\n",
                ")\n",
                "\n",
                "print(f\"Original dtype: {data.dtype}\")\n",
                "print(f\"Quantized dtype: {quantized.dtype}\")\n",
                "print(f\"Scale: {scale:.6f}, Zero point: {zero_point}\")\n",
                "print(f\"Size reduction: {data.nbytes / quantized.nbytes:.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Zenith + PyTorch (Hybrid Mode)\n",
                "\n",
                "Zenith bekerja sebagai **pelengkap** PyTorch untuk optimasi model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import zenith\n",
                "\n",
                "# 3.1 Buat model PyTorch\n",
                "class SimpleTransformer(nn.Module):\n",
                "    def __init__(self, d_model=256, nhead=4, num_layers=2):\n",
                "        super().__init__()\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=d_model, nhead=nhead, batch_first=True\n",
                "        )\n",
                "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        self.fc = nn.Linear(d_model, 10)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.encoder(x)\n",
                "        return self.fc(x.mean(dim=1))\n",
                "\n",
                "# Create model\n",
                "model = SimpleTransformer().cuda().eval()\n",
                "print(f\"Model created: {type(model).__name__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2 Compile dengan Zenith\n",
                "# PENTING: sample_input diperlukan untuk tracing PyTorch model\n",
                "x = torch.randn(8, 32, 256).cuda()  # batch=8, seq=32, d_model=256\n",
                "\n",
                "optimized = zenith.compile(\n",
                "    model,\n",
                "    target=\"cuda\",\n",
                "    precision=\"fp32\",\n",
                "    sample_input=x  # Required untuk PyTorch model\n",
                ")\n",
                "\n",
                "print(f\"Compiled model: {type(optimized).__name__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.3 Inference\n",
                "with torch.no_grad():\n",
                "    output = optimized(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.4 Benchmark: PyTorch vs PyTorch+Zenith\n",
                "import time\n",
                "\n",
                "def benchmark(model, x, name, warmup=10, runs=50):\n",
                "    # Warmup\n",
                "    with torch.no_grad():\n",
                "        for _ in range(warmup):\n",
                "            _ = model(x)\n",
                "    \n",
                "    torch.cuda.synchronize()\n",
                "    \n",
                "    # Benchmark\n",
                "    times = []\n",
                "    with torch.no_grad():\n",
                "        for _ in range(runs):\n",
                "            torch.cuda.synchronize()\n",
                "            start = time.perf_counter()\n",
                "            _ = model(x)\n",
                "            torch.cuda.synchronize()\n",
                "            times.append((time.perf_counter() - start) * 1000)\n",
                "    \n",
                "    avg = sum(times) / len(times)\n",
                "    print(f\"{name}: {avg:.2f} ms\")\n",
                "    return avg\n",
                "\n",
                "# Benchmark\n",
                "print(\"\\n=== BENCHMARK ===\")\n",
                "t1 = benchmark(model, x, \"Pure PyTorch\")\n",
                "t2 = benchmark(optimized, x, \"Zenith + PyTorch\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 FP16 Mode (Tensor Core)\n",
                "\n",
                "FP16 memberikan speedup signifikan pada GPU dengan Tensor Cores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FP16 untuk Tensor Core acceleration\n",
                "model_fp16 = SimpleTransformer().cuda().half().eval()\n",
                "\n",
                "x_fp16 = torch.randn(8, 32, 256, dtype=torch.float16).cuda()\n",
                "\n",
                "print(\"\\n=== FP16 BENCHMARK (Tensor Core) ===\")\n",
                "t_fp16 = benchmark(model_fp16, x_fp16, \"FP16 PyTorch\")\n",
                "print(f\"\\nSpeedup FP32 -> FP16: {t1/t_fp16:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Zenith + TensorFlow (Hybrid Mode)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install TensorFlow support (opsional)\n",
                "# !pip install pyzenith[tensorflow] -q\n",
                "\n",
                "try:\n",
                "    import tensorflow as tf\n",
                "    \n",
                "    # 4.1 Buat model TensorFlow\n",
                "    tf_model = tf.keras.Sequential([\n",
                "        tf.keras.layers.Dense(256, activation='relu', input_shape=(784,)),\n",
                "        tf.keras.layers.Dense(128, activation='relu'),\n",
                "        tf.keras.layers.Dense(10, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    print(f\"TensorFlow model: {tf_model.name}\")\n",
                "    print(f\"Total params: {tf_model.count_params():,}\")\n",
                "    \n",
                "    # TensorFlow uses eager execution, no sample_input needed\n",
                "    print(\"\\nNote: TensorFlow models work with Zenith via ONNX export.\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"TensorFlow not installed. Skip this section.\")\n",
                "    print(\"To enable: pip install pyzenith[tensorflow]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Zenith + JAX (Hybrid Mode)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# JAX sudah tersedia di Colab\n",
                "try:\n",
                "    import jax\n",
                "    import jax.numpy as jnp\n",
                "    \n",
                "    # 5.1 Buat fungsi JAX\n",
                "    def jax_mlp(params, x):\n",
                "        for w, b in params[:-1]:\n",
                "            x = jax.nn.relu(jnp.dot(x, w) + b)\n",
                "        w, b = params[-1]\n",
                "        return jnp.dot(x, w) + b\n",
                "    \n",
                "    # Initialize params\n",
                "    key = jax.random.PRNGKey(0)\n",
                "    params = [\n",
                "        (jax.random.normal(key, (784, 256)) * 0.01, jnp.zeros(256)),\n",
                "        (jax.random.normal(key, (256, 128)) * 0.01, jnp.zeros(128)),\n",
                "        (jax.random.normal(key, (128, 10)) * 0.01, jnp.zeros(10)),\n",
                "    ]\n",
                "    \n",
                "    # JIT compile\n",
                "    jax_mlp_jit = jax.jit(jax_mlp)\n",
                "    \n",
                "    # Test\n",
                "    x_jax = jax.random.normal(key, (32, 784))\n",
                "    output_jax = jax_mlp_jit(params, x_jax)\n",
                "    print(f\"JAX output shape: {output_jax.shape}\")\n",
                "    \n",
                "    print(\"\\nNote: Zenith JAX integration is via optimization passes.\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"JAX not installed. Skip this section.\")\n",
                "    print(\"To enable: pip install pyzenith[jax]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Zenith Full Performance Mode\n",
                "\n",
                "Untuk performa maksimal, kombinasikan:\n",
                "1. **FP16 Precision** - Tensor Core utilization\n",
                "2. **Operator Fusion** - Reduce memory bandwidth\n",
                "3. **Quantization** - Model size reduction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.1 Full Performance Example\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "class BERTStyleBlock(nn.Module):\n",
                "    \"\"\"BERT-style Transformer block\"\"\"\n",
                "    def __init__(self, d_model=768, nhead=12):\n",
                "        super().__init__()\n",
                "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
                "        self.ln1 = nn.LayerNorm(d_model)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(d_model, d_model * 4),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(d_model * 4, d_model)\n",
                "        )\n",
                "        self.ln2 = nn.LayerNorm(d_model)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Self-attention + residual\n",
                "        attn_out, _ = self.attn(x, x, x)\n",
                "        x = self.ln1(x + attn_out)\n",
                "        # FFN + residual\n",
                "        ffn_out = self.ffn(x)\n",
                "        x = self.ln2(x + ffn_out)\n",
                "        return x\n",
                "\n",
                "# Create 4-layer BERT-style model\n",
                "class MiniTransformer(nn.Module):\n",
                "    def __init__(self, num_layers=4):\n",
                "        super().__init__()\n",
                "        self.layers = nn.ModuleList([BERTStyleBlock() for _ in range(num_layers)])\n",
                "        self.pooler = nn.Linear(768, 768)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        for layer in self.layers:\n",
                "            x = layer(x)\n",
                "        return torch.tanh(self.pooler(x[:, 0]))\n",
                "\n",
                "print(\"Created MiniTransformer (4 layers, 768 dim)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.2 Compare FP32 vs FP16 Full Performance\n",
                "\n",
                "# FP32 Model\n",
                "model_fp32 = MiniTransformer().cuda().eval()\n",
                "\n",
                "# FP16 Model (Tensor Core)\n",
                "model_fp16 = MiniTransformer().cuda().half().eval()\n",
                "\n",
                "# Input\n",
                "batch_size = 8\n",
                "seq_len = 128\n",
                "d_model = 768\n",
                "\n",
                "x_fp32 = torch.randn(batch_size, seq_len, d_model).cuda()\n",
                "x_fp16 = x_fp32.half()\n",
                "\n",
                "print(f\"Input: batch={batch_size}, seq={seq_len}, d_model={d_model}\")\n",
                "print(f\"Total params: {sum(p.numel() for p in model_fp32.parameters()):,}\")\n",
                "\n",
                "# Benchmark\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"FULL PERFORMANCE BENCHMARK\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "t_fp32 = benchmark(model_fp32, x_fp32, \"FP32 (CUDA Cores)\")\n",
                "t_fp16 = benchmark(model_fp16, x_fp16, \"FP16 (Tensor Cores)\")\n",
                "\n",
                "print(\"\\n\" + \"-\"*50)\n",
                "print(f\"Speedup: {t_fp32/t_fp16:.2f}x\")\n",
                "print(f\"Memory: FP32={x_fp32.nbytes/1024:.1f}KB, FP16={x_fp16.nbytes/1024:.1f}KB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.3 Quantization Performance\n",
                "from zenith.optimization.quantization import quantize, dequantize\n",
                "\n",
                "# Get model weights\n",
                "total_fp32 = 0\n",
                "total_int8 = 0\n",
                "\n",
                "for name, param in model_fp32.named_parameters():\n",
                "    if 'weight' in name and param.ndim >= 2:\n",
                "        weights = param.detach().cpu().numpy()\n",
                "        quantized, scale, zp = quantize(weights)\n",
                "        \n",
                "        total_fp32 += weights.nbytes\n",
                "        total_int8 += quantized.nbytes\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"QUANTIZATION RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"FP32 model size: {total_fp32/1024/1024:.2f} MB\")\n",
                "print(f\"INT8 model size: {total_int8/1024/1024:.2f} MB\")\n",
                "print(f\"Compression: {total_fp32/total_int8:.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "| Mode | Use Case | Performance |\n",
                "|------|----------|-------------|\n",
                "| `zenith.compile(model, sample_input=x)` | Quick optimization | Baseline |\n",
                "| `model.half()` (FP16) | Inference on GPU | 2-6x faster |\n",
                "| `quantize()` (INT8) | Edge deployment | 4x smaller |\n",
                "| FP16 + Zenith | Production inference | Best |\n",
                "\n",
                "### Recommended Usage\n",
                "\n",
                "```python\n",
                "import zenith\n",
                "import torch\n",
                "\n",
                "# 1. Load model\n",
                "model = MyModel().cuda().half().eval()\n",
                "\n",
                "# 2. Create sample input\n",
                "sample = torch.randn(1, 768).cuda().half()\n",
                "\n",
                "# 3. Compile with Zenith\n",
                "optimized = zenith.compile(\n",
                "    model,\n",
                "    target=\"cuda\",\n",
                "    precision=\"fp16\",\n",
                "    sample_input=sample\n",
                ")\n",
                "\n",
                "# 4. Run inference\n",
                "with torch.no_grad():\n",
                "    output = optimized(input.half())\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ZENITH TUTORIAL COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nZenith Version: {zenith.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
                "print(\"\\nModules tested:\")\n",
                "print(\"  - zenith.compile() - OK\")\n",
                "print(\"  - zenith.optimization.quantization - OK\")\n",
                "print(\"  - FP16 Tensor Core - OK\")\n",
                "print(\"\\nFor more info: https://github.com/vibeswithkk/ZENITH\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}