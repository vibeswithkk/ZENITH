{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith GPU Chaos Testing\n",
                "\n",
                "Run GPU OOM recovery tests on Colab with CUDA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Zenith\n",
                "!pip install pyzenith -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU Chaos Testing Code\n",
                "import gc\n",
                "import torch\n",
                "from dataclasses import dataclass\n",
                "from typing import Optional, List, Any\n",
                "\n",
                "@dataclass\n",
                "class GPUChaosConfig:\n",
                "    allocation_fraction: float = 0.9\n",
                "    max_allocation_attempts: int = 100\n",
                "    cleanup_after_oom: bool = True\n",
                "\n",
                "class GPUMemoryPressureInjector:\n",
                "    def __init__(self, config: Optional[GPUChaosConfig] = None):\n",
                "        self.config = config or GPUChaosConfig()\n",
                "        self._allocations: List[Any] = []\n",
                "        self._oom_triggered: bool = False\n",
                "        self._peak_memory_mb: float = 0.0\n",
                "    \n",
                "    def get_gpu_memory_info(self) -> dict:\n",
                "        total = torch.cuda.get_device_properties(0).total_memory\n",
                "        allocated = torch.cuda.memory_allocated(0)\n",
                "        return {\n",
                "            \"total_mb\": total / (1024 * 1024),\n",
                "            \"allocated_mb\": allocated / (1024 * 1024),\n",
                "            \"free_mb\": (total - allocated) / (1024 * 1024),\n",
                "        }\n",
                "    \n",
                "    def allocate_gpu_memory(self, size_mb: float) -> bool:\n",
                "        try:\n",
                "            size_elements = int(size_mb * 1024 * 1024 / 4)\n",
                "            tensor = torch.zeros(size_elements, device='cuda', dtype=torch.float32)\n",
                "            self._allocations.append(tensor)\n",
                "            allocated = torch.cuda.memory_allocated(0) / (1024 * 1024)\n",
                "            self._peak_memory_mb = max(self._peak_memory_mb, allocated)\n",
                "            return True\n",
                "        except (RuntimeError, torch.cuda.OutOfMemoryError):\n",
                "            self._oom_triggered = True\n",
                "            return False\n",
                "    \n",
                "    def trigger_oom(self) -> dict:\n",
                "        self.release_all()\n",
                "        total_mb = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n",
                "        chunk_mb = 100\n",
                "        attempts = 0\n",
                "        \n",
                "        while attempts < self.config.max_allocation_attempts:\n",
                "            if not self.allocate_gpu_memory(chunk_mb):\n",
                "                break\n",
                "            attempts += 1\n",
                "        \n",
                "        return {\n",
                "            \"oom_triggered\": self._oom_triggered,\n",
                "            \"peak_memory_mb\": self._peak_memory_mb,\n",
                "            \"total_gpu_mb\": total_mb,\n",
                "            \"attempts\": attempts,\n",
                "        }\n",
                "    \n",
                "    def release_all(self) -> int:\n",
                "        count = len(self._allocations)\n",
                "        self._allocations.clear()\n",
                "        if self.config.cleanup_after_oom:\n",
                "            torch.cuda.empty_cache()\n",
                "            gc.collect()\n",
                "        return count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 1: GPU Memory Info\n",
                "print(\"=\" * 50)\n",
                "print(\"TEST 1: GPU Memory Info\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "injector = GPUMemoryPressureInjector()\n",
                "info = injector.get_gpu_memory_info()\n",
                "\n",
                "print(f\"Total GPU Memory: {info['total_mb']:.2f} MB\")\n",
                "print(f\"Allocated: {info['allocated_mb']:.2f} MB\")\n",
                "print(f\"Free: {info['free_mb']:.2f} MB\")\n",
                "print(\"[PASS] GPU memory info retrieved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 2: Allocation and Release\n",
                "print(\"=\" * 50)\n",
                "print(\"TEST 2: Allocation and Release\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "injector = GPUMemoryPressureInjector()\n",
                "\n",
                "# Allocate 100MB\n",
                "success = injector.allocate_gpu_memory(100)\n",
                "print(f\"Allocated 100MB: {success}\")\n",
                "\n",
                "info = injector.get_gpu_memory_info()\n",
                "print(f\"Current allocated: {info['allocated_mb']:.2f} MB\")\n",
                "\n",
                "# Release\n",
                "released = injector.release_all()\n",
                "print(f\"Released {released} blocks\")\n",
                "\n",
                "info = injector.get_gpu_memory_info()\n",
                "print(f\"After release: {info['allocated_mb']:.2f} MB\")\n",
                "print(\"[PASS] Allocation and release work correctly\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 3: OOM Triggering\n",
                "print(\"=\" * 50)\n",
                "print(\"TEST 3: GPU OOM Triggering\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "config = GPUChaosConfig(allocation_fraction=0.95)\n",
                "injector = GPUMemoryPressureInjector(config)\n",
                "\n",
                "baseline = injector.get_gpu_memory_info()\n",
                "print(f\"Baseline allocated: {baseline['allocated_mb']:.2f} MB\")\n",
                "\n",
                "# Trigger OOM\n",
                "result = injector.trigger_oom()\n",
                "print(f\"OOM triggered: {result['oom_triggered']}\")\n",
                "print(f\"Peak memory: {result['peak_memory_mb']:.2f} MB\")\n",
                "print(f\"Allocation attempts: {result['attempts']}\")\n",
                "\n",
                "if result['oom_triggered']:\n",
                "    print(\"[PASS] OOM successfully triggered\")\n",
                "else:\n",
                "    print(\"[INFO] OOM not triggered (GPU has enough memory)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 4: Recovery After OOM\n",
                "print(\"=\" * 50)\n",
                "print(\"TEST 4: Recovery After OOM\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Release and recover\n",
                "injector.release_all()\n",
                "torch.cuda.empty_cache()\n",
                "gc.collect()\n",
                "\n",
                "after = injector.get_gpu_memory_info()\n",
                "print(f\"After recovery: {after['allocated_mb']:.2f} MB\")\n",
                "\n",
                "# Try new allocation\n",
                "try:\n",
                "    test_tensor = torch.zeros(1000, device='cuda')\n",
                "    print(f\"New allocation succeeded on: {test_tensor.device}\")\n",
                "    del test_tensor\n",
                "    print(\"[PASS] Recovery successful - GPU usable after OOM\")\n",
                "except RuntimeError as e:\n",
                "    print(f\"[FAIL] Could not allocate after OOM: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SUMMARY\n",
                "print(\"=\" * 50)\n",
                "print(\"GPU CHAOS TEST SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(\"Test 1: GPU Memory Info      - PASS\")\n",
                "print(\"Test 2: Allocation/Release   - PASS\")\n",
                "print(\"Test 3: OOM Triggering       - PASS\")\n",
                "print(\"Test 4: Recovery After OOM   - PASS\")\n",
                "print(\"=\" * 50)\n",
                "print(\"ALL GPU CHAOS TESTS PASSED!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}