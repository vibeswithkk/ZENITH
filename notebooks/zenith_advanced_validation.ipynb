{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Advanced Features Validation\n",
                "\n",
                "**Real-world validation of:**\n",
                "1. CUDA Graphs\n",
                "2. Gradient Checkpointing (Phase 1 & 2)\n",
                "3. Complete E2E Inference\n",
                "\n",
                "**Important:** This notebook uses REAL tests that verify actual functionality,\n",
                "not mock tests that always pass.\n",
                "\n",
                "**Requirements:** GPU Runtime (Runtime > Change runtime type > T4 GPU)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install pyzenith v0.2.9 (with memory and inference modules)\n",
                "!pip install -q pyzenith==0.2.9 torch numpy\n",
                "\n",
                "# Verify GPU is available\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"CUDA version: {torch.version.cuda}\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU detected. CUDA Graphs tests will be skipped.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Zenith\n",
                "import zenith\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "\n",
                "# Test basic import\n",
                "from zenith.memory import gradient_checkpointing\n",
                "from zenith.memory import native_checkpointing\n",
                "from zenith import inference\n",
                "print(\"All modules imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. CUDA Graphs Validation\n",
                "\n",
                "**What we're testing:**\n",
                "- CUDA Graphs capture and replay\n",
                "- Performance improvement (should be measurably faster)\n",
                "- Numerical correctness (outputs must match non-graph execution)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import numpy as np\n",
                "\n",
                "def test_cuda_graphs():\n",
                "    \"\"\"Test CUDA Graphs with real performance measurement.\"\"\"\n",
                "    \n",
                "    if not torch.cuda.is_available():\n",
                "        print(\"SKIPPED: No GPU available\")\n",
                "        return None\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    print(\"TEST: CUDA Graphs\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Create a model that benefits from CUDA Graphs\n",
                "    model = torch.nn.Sequential(\n",
                "        torch.nn.Linear(512, 256),\n",
                "        torch.nn.ReLU(),\n",
                "        torch.nn.Linear(256, 128),\n",
                "        torch.nn.ReLU(),\n",
                "        torch.nn.Linear(128, 64),\n",
                "    ).cuda()\n",
                "    \n",
                "    # Fixed input for CUDA Graphs (shape must be constant)\n",
                "    x = torch.randn(32, 512, device='cuda')\n",
                "    \n",
                "    # Warmup\n",
                "    for _ in range(10):\n",
                "        _ = model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    \n",
                "    # --- Baseline: Without CUDA Graphs ---\n",
                "    num_runs = 100\n",
                "    torch.cuda.synchronize()\n",
                "    start = time.perf_counter()\n",
                "    for _ in range(num_runs):\n",
                "        output_baseline = model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    baseline_time = (time.perf_counter() - start) * 1000  # ms\n",
                "    \n",
                "    print(f\"\\nBaseline (no CUDA Graphs): {baseline_time:.2f}ms for {num_runs} runs\")\n",
                "    print(f\"  Per-iteration: {baseline_time/num_runs:.3f}ms\")\n",
                "    \n",
                "    # --- With CUDA Graphs ---\n",
                "    # Capture graph\n",
                "    s = torch.cuda.Stream()\n",
                "    s.wait_stream(torch.cuda.current_stream())\n",
                "    \n",
                "    with torch.cuda.stream(s):\n",
                "        # Warmup in capture stream\n",
                "        for _ in range(3):\n",
                "            _ = model(x)\n",
                "    torch.cuda.current_stream().wait_stream(s)\n",
                "    \n",
                "    # Capture\n",
                "    g = torch.cuda.CUDAGraph()\n",
                "    static_input = x.clone()\n",
                "    \n",
                "    with torch.cuda.graph(g):\n",
                "        static_output = model(static_input)\n",
                "    \n",
                "    # Replay timing\n",
                "    torch.cuda.synchronize()\n",
                "    start = time.perf_counter()\n",
                "    for _ in range(num_runs):\n",
                "        g.replay()\n",
                "    torch.cuda.synchronize()\n",
                "    graph_time = (time.perf_counter() - start) * 1000  # ms\n",
                "    \n",
                "    print(f\"\\nCUDA Graphs: {graph_time:.2f}ms for {num_runs} runs\")\n",
                "    print(f\"  Per-iteration: {graph_time/num_runs:.3f}ms\")\n",
                "    \n",
                "    # --- Verify Correctness ---\n",
                "    # Run once more with fresh data\n",
                "    static_input.copy_(x)\n",
                "    g.replay()\n",
                "    torch.cuda.synchronize()\n",
                "    \n",
                "    output_graph = static_output.clone()\n",
                "    output_baseline = model(x)\n",
                "    \n",
                "    max_diff = (output_graph - output_baseline).abs().max().item()\n",
                "    \n",
                "    print(f\"\\n--- Verification ---\")\n",
                "    print(f\"Max difference between graph and baseline: {max_diff:.2e}\")\n",
                "    \n",
                "    # Results\n",
                "    speedup = baseline_time / graph_time\n",
                "    print(f\"\\n--- Results ---\")\n",
                "    print(f\"Speedup: {speedup:.2f}x\")\n",
                "    \n",
                "    # Assertions\n",
                "    if max_diff < 1e-5:\n",
                "        print(\"NUMERICAL CORRECTNESS: PASSED\")\n",
                "    else:\n",
                "        print(f\"NUMERICAL CORRECTNESS: FAILED (diff={max_diff})\")\n",
                "        return False\n",
                "    \n",
                "    if speedup > 1.0:\n",
                "        print(f\"PERFORMANCE: PASSED (speedup={speedup:.2f}x)\")\n",
                "    else:\n",
                "        print(f\"PERFORMANCE: MARGINAL (speedup={speedup:.2f}x)\")\n",
                "    \n",
                "    return True\n",
                "\n",
                "cuda_graphs_passed = test_cuda_graphs()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Gradient Checkpointing Phase 1 (PyTorch-based)\n",
                "\n",
                "**What we're testing:**\n",
                "- Memory reduction during training\n",
                "- Gradient correctness (must match non-checkpointed version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.memory.gradient_checkpointing import checkpoint, checkpoint_sequential\n",
                "\n",
                "def test_gradient_checkpointing_phase1():\n",
                "    \"\"\"Test Phase 1 Gradient Checkpointing with real gradient verification.\"\"\"\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    print(\"TEST: Gradient Checkpointing Phase 1\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    \n",
                "    # Create a model with multiple layers\n",
                "    class DeepModel(torch.nn.Module):\n",
                "        def __init__(self):\n",
                "            super().__init__()\n",
                "            self.layers = torch.nn.ModuleList([\n",
                "                torch.nn.Linear(256, 256) for _ in range(8)\n",
                "            ])\n",
                "            self.activation = torch.nn.ReLU()\n",
                "        \n",
                "        def forward(self, x, use_checkpoint=False):\n",
                "            for layer in self.layers:\n",
                "                if use_checkpoint:\n",
                "                    x = checkpoint(lambda inp: self.activation(layer(inp)), x)\n",
                "                else:\n",
                "                    x = self.activation(layer(x))\n",
                "            return x\n",
                "    \n",
                "    model = DeepModel().to(device)\n",
                "    \n",
                "    # Test input (same for both runs)\n",
                "    torch.manual_seed(42)\n",
                "    x = torch.randn(16, 256, device=device, requires_grad=True)\n",
                "    \n",
                "    # --- Run WITHOUT checkpointing ---\n",
                "    x_no_ckpt = x.clone().detach().requires_grad_(True)\n",
                "    output_no_ckpt = model(x_no_ckpt, use_checkpoint=False)\n",
                "    loss_no_ckpt = output_no_ckpt.sum()\n",
                "    loss_no_ckpt.backward()\n",
                "    grad_no_ckpt = x_no_ckpt.grad.clone()\n",
                "    \n",
                "    # --- Run WITH checkpointing ---\n",
                "    model.zero_grad()\n",
                "    x_ckpt = x.clone().detach().requires_grad_(True)\n",
                "    output_ckpt = model(x_ckpt, use_checkpoint=True)\n",
                "    loss_ckpt = output_ckpt.sum()\n",
                "    loss_ckpt.backward()\n",
                "    grad_ckpt = x_ckpt.grad.clone()\n",
                "    \n",
                "    # --- Verify Gradient Correctness ---\n",
                "    grad_diff = (grad_no_ckpt - grad_ckpt).abs().max().item()\n",
                "    \n",
                "    print(f\"\\n--- Gradient Verification ---\")\n",
                "    print(f\"Max gradient difference: {grad_diff:.2e}\")\n",
                "    \n",
                "    if grad_diff < 1e-5:\n",
                "        print(\"GRADIENT CORRECTNESS: PASSED\")\n",
                "        return True\n",
                "    else:\n",
                "        print(f\"GRADIENT CORRECTNESS: FAILED (diff={grad_diff})\")\n",
                "        return False\n",
                "\n",
                "phase1_passed = test_gradient_checkpointing_phase1()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Gradient Checkpointing Phase 2 (Native Implementation)\n",
                "\n",
                "**What we're testing:**\n",
                "- Native checkpointing with activation store\n",
                "- Optimal checkpoint selection (DP algorithm)\n",
                "- Memory tracking accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.memory.native_checkpointing import (\n",
                "    native_checkpoint,\n",
                "    native_checkpoint_sequential,\n",
                "    NativeCheckpointer,\n",
                "    OptimalCheckpointSelector,\n",
                "    ActivationStore\n",
                ")\n",
                "\n",
                "def test_native_checkpointing():\n",
                "    \"\"\"Test Phase 2 Native Gradient Checkpointing.\"\"\"\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    print(\"TEST: Gradient Checkpointing Phase 2 (Native)\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    all_passed = True\n",
                "    \n",
                "    # --- Test 1: Basic native_checkpoint ---\n",
                "    print(\"\\n[1] Testing native_checkpoint function...\")\n",
                "    \n",
                "    def simple_fn(x):\n",
                "        return x * 2 + 1\n",
                "    \n",
                "    x = torch.randn(4, 4, device=device, requires_grad=True)\n",
                "    \n",
                "    # Without checkpoint\n",
                "    x1 = x.clone().detach().requires_grad_(True)\n",
                "    out1 = simple_fn(x1)\n",
                "    out1.sum().backward()\n",
                "    grad1 = x1.grad.clone()\n",
                "    \n",
                "    # With native checkpoint\n",
                "    x2 = x.clone().detach().requires_grad_(True)\n",
                "    out2 = native_checkpoint(simple_fn, x2)\n",
                "    out2.sum().backward()\n",
                "    grad2 = x2.grad.clone()\n",
                "    \n",
                "    diff = (grad1 - grad2).abs().max().item()\n",
                "    if diff < 1e-6:\n",
                "        print(f\"  PASSED (gradient diff: {diff:.2e})\")\n",
                "    else:\n",
                "        print(f\"  FAILED (gradient diff: {diff:.2e})\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 2: Sequential checkpointing ---\n",
                "    print(\"\\n[2] Testing native_checkpoint_sequential...\")\n",
                "    \n",
                "    layers = torch.nn.ModuleList([\n",
                "        torch.nn.Linear(64, 64) for _ in range(4)\n",
                "    ]).to(device)\n",
                "    \n",
                "    x = torch.randn(8, 64, device=device, requires_grad=True)\n",
                "    \n",
                "    # Without checkpoint\n",
                "    x1 = x.clone().detach().requires_grad_(True)\n",
                "    out1 = x1\n",
                "    for layer in layers:\n",
                "        out1 = layer(out1)\n",
                "    out1.sum().backward()\n",
                "    grad1 = x1.grad.clone()\n",
                "    \n",
                "    # Reset grads\n",
                "    for layer in layers:\n",
                "        if layer.weight.grad is not None:\n",
                "            layer.weight.grad.zero_()\n",
                "            layer.bias.grad.zero_()\n",
                "    \n",
                "    # With sequential checkpoint - use keyword args correctly\n",
                "    x2 = x.clone().detach().requires_grad_(True)\n",
                "    out2 = native_checkpoint_sequential(\n",
                "        functions=list(layers),\n",
                "        segments=2,\n",
                "        input_tensor=x2\n",
                "    )\n",
                "    out2.sum().backward()\n",
                "    grad2 = x2.grad.clone()\n",
                "    \n",
                "    diff = (grad1 - grad2).abs().max().item()\n",
                "    if diff < 1e-4:\n",
                "        print(f\"  PASSED (gradient diff: {diff:.2e})\")\n",
                "    else:\n",
                "        print(f\"  FAILED (gradient diff: {diff:.2e})\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 3: OptimalCheckpointSelector ---\n",
                "    print(\"\\n[3] Testing OptimalCheckpointSelector (DP algorithm)...\")\n",
                "    \n",
                "    selector = OptimalCheckpointSelector(\n",
                "        num_layers=10,\n",
                "        memory_costs=[1.0] * 10,\n",
                "        compute_costs=[1.0] * 10,\n",
                "    )\n",
                "    \n",
                "    # Test sqrt heuristic\n",
                "    ckpts_sqrt = selector.select_checkpoints_sqrt()\n",
                "    print(f\"  Sqrt heuristic: {len(ckpts_sqrt)} checkpoints at {ckpts_sqrt}\")\n",
                "    \n",
                "    # Test DP algorithm (no arguments)\n",
                "    ckpts_dp = selector.select_checkpoints_dp()\n",
                "    print(f\"  DP algorithm: {len(ckpts_dp)} checkpoints at {ckpts_dp}\")\n",
                "    \n",
                "    if len(ckpts_sqrt) > 0 and len(ckpts_dp) > 0:\n",
                "        print(\"  PASSED\")\n",
                "    else:\n",
                "        print(\"  WARNING: No checkpoints selected\")\n",
                "    \n",
                "    # --- Test 4: ActivationStore ---\n",
                "    print(\"\\n[4] Testing ActivationStore...\")\n",
                "    \n",
                "    # max_memory_bytes (10 MB = 10 * 1024 * 1024 bytes)\n",
                "    store = ActivationStore(max_memory_bytes=10 * 1024 * 1024)\n",
                "    \n",
                "    # Store some tensors\n",
                "    t1 = torch.randn(100, 100, device=device)\n",
                "    t2 = torch.randn(100, 100, device=device)\n",
                "    \n",
                "    store.store(0, t1)  # layer_id = 0\n",
                "    store.store(1, t2)  # layer_id = 1\n",
                "    \n",
                "    # Retrieve\n",
                "    r1 = store.get(0)\n",
                "    r2 = store.get(1)\n",
                "    \n",
                "    if r1 is not None and r2 is not None:\n",
                "        if torch.allclose(t1, r1) and torch.allclose(t2, r2):\n",
                "            print(\"  PASSED (store/retrieve works correctly)\")\n",
                "        else:\n",
                "            print(\"  FAILED (data mismatch)\")\n",
                "            all_passed = False\n",
                "    else:\n",
                "        print(\"  FAILED (tensors not found)\")\n",
                "        all_passed = False\n",
                "    \n",
                "    print(f\"\\n--- Summary ---\")\n",
                "    if all_passed:\n",
                "        print(\"ALL NATIVE CHECKPOINTING TESTS PASSED\")\n",
                "    else:\n",
                "        print(\"SOME TESTS FAILED\")\n",
                "    \n",
                "    return all_passed\n",
                "\n",
                "phase2_passed = test_native_checkpointing()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Complete E2E Inference\n",
                "\n",
                "**What we're testing:**\n",
                "- InferenceSession creation and execution\n",
                "- Benchmark API accuracy\n",
                "- Output correctness (must match PyTorch native output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from zenith.inference import (\n",
                "    InferenceSession,\n",
                "    InferenceConfig,\n",
                "    InferenceStats,\n",
                "    InferenceResult,\n",
                "    create_session,\n",
                "    infer\n",
                ")\n",
                "\n",
                "def test_e2e_inference():\n",
                "    \"\"\"Test Complete E2E Inference Pipeline.\"\"\"\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    print(\"TEST: Complete E2E Inference\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    all_passed = True\n",
                "    \n",
                "    # --- Test 1: Basic Session Creation ---\n",
                "    print(\"\\n[1] Testing InferenceSession creation...\")\n",
                "    \n",
                "    model = torch.nn.Sequential(\n",
                "        torch.nn.Linear(128, 64),\n",
                "        torch.nn.ReLU(),\n",
                "        torch.nn.Linear(64, 32),\n",
                "    ).to(device)\n",
                "    \n",
                "    sample_input = torch.randn(4, 128, device=device)\n",
                "    \n",
                "    config = InferenceConfig(\n",
                "        backend=device,\n",
                "        verbose=0,\n",
                "        warmup_iterations=2\n",
                "    )\n",
                "    \n",
                "    session = InferenceSession(model, config=config, sample_input=sample_input)\n",
                "    \n",
                "    if session.is_initialized:\n",
                "        print(f\"  Session created: framework={session.framework}, backend={session.backend}\")\n",
                "        print(\"  PASSED\")\n",
                "    else:\n",
                "        print(\"  FAILED: Session not initialized\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 2: Inference Run ---\n",
                "    print(\"\\n[2] Testing inference run...\")\n",
                "    \n",
                "    test_input = torch.randn(4, 128, device=device)\n",
                "    \n",
                "    # Run via session\n",
                "    result = session.run({'input': test_input})\n",
                "    \n",
                "    # Run via native PyTorch\n",
                "    with torch.no_grad():\n",
                "        expected = model(test_input)\n",
                "    \n",
                "    # Compare\n",
                "    if isinstance(result, dict):\n",
                "        out_tensor = list(result.values())[0]\n",
                "        if isinstance(out_tensor, np.ndarray):\n",
                "            out_tensor = torch.from_numpy(out_tensor).to(device)\n",
                "        elif hasattr(out_tensor, 'cpu'):\n",
                "            out_tensor = out_tensor.to(device)\n",
                "    else:\n",
                "        out_tensor = result\n",
                "    \n",
                "    diff = (out_tensor.float() - expected.float()).abs().max().item()\n",
                "    \n",
                "    if diff < 1e-4:\n",
                "        print(f\"  Output matches native PyTorch (diff: {diff:.2e})\")\n",
                "        print(\"  PASSED\")\n",
                "    else:\n",
                "        print(f\"  FAILED: Output mismatch (diff: {diff:.2e})\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 3: Latency Return ---\n",
                "    print(\"\\n[3] Testing latency measurement...\")\n",
                "    \n",
                "    result_with_latency = session.run({'input': test_input}, return_latency=True)\n",
                "    \n",
                "    if isinstance(result_with_latency, InferenceResult):\n",
                "        print(f\"  Latency: {result_with_latency.latency_ms:.3f}ms\")\n",
                "        print(f\"  Backend: {result_with_latency.backend_used}\")\n",
                "        print(\"  PASSED\")\n",
                "    else:\n",
                "        print(\"  FAILED: Expected InferenceResult\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 4: Statistics ---\n",
                "    print(\"\\n[4] Testing statistics tracking...\")\n",
                "    \n",
                "    # Run 10 more times\n",
                "    for _ in range(10):\n",
                "        session.run({'input': test_input})\n",
                "    \n",
                "    stats = session.get_stats()\n",
                "    \n",
                "    print(f\"  Total runs: {stats['total_runs']}\")\n",
                "    print(f\"  Mean latency: {stats['mean_latency_ms']:.3f}ms\")\n",
                "    print(f\"  Min latency: {stats['min_latency_ms']:.3f}ms\")\n",
                "    print(f\"  Max latency: {stats['max_latency_ms']:.3f}ms\")\n",
                "    \n",
                "    if stats['total_runs'] >= 10 and stats['mean_latency_ms'] > 0:\n",
                "        print(\"  PASSED\")\n",
                "    else:\n",
                "        print(\"  FAILED\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 5: Benchmark API ---\n",
                "    print(\"\\n[5] Testing benchmark API...\")\n",
                "    \n",
                "    bench = session.benchmark({'input': test_input}, num_runs=20, num_warmup=5)\n",
                "    \n",
                "    print(f\"  Benchmark results:\")\n",
                "    print(f\"    Mean: {bench['mean_ms']:.3f}ms\")\n",
                "    print(f\"    P50:  {bench['p50_ms']:.3f}ms\")\n",
                "    print(f\"    P99:  {bench['p99_ms']:.3f}ms\")\n",
                "    print(f\"    Throughput: {bench['throughput_per_sec']:.1f}/sec\")\n",
                "    \n",
                "    required_keys = ['mean_ms', 'std_ms', 'min_ms', 'max_ms', 'p50_ms', 'p90_ms', 'p99_ms']\n",
                "    if all(k in bench for k in required_keys):\n",
                "        print(\"  PASSED\")\n",
                "    else:\n",
                "        print(\"  FAILED: Missing benchmark keys\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # --- Test 6: Convenience Functions ---\n",
                "    print(\"\\n[6] Testing convenience functions...\")\n",
                "    \n",
                "    # create_session\n",
                "    session2 = create_session(model, config=config, sample_input=sample_input)\n",
                "    if session2.is_initialized:\n",
                "        print(\"  create_session: PASSED\")\n",
                "    else:\n",
                "        print(\"  create_session: FAILED\")\n",
                "        all_passed = False\n",
                "    \n",
                "    # infer (one-shot)\n",
                "    result = infer(model, {'input': test_input}, config=config)\n",
                "    if result is not None and len(result) > 0:\n",
                "        print(\"  infer: PASSED\")\n",
                "    else:\n",
                "        print(\"  infer: FAILED\")\n",
                "        all_passed = False\n",
                "    \n",
                "    print(f\"\\n--- Summary ---\")\n",
                "    if all_passed:\n",
                "        print(\"ALL E2E INFERENCE TESTS PASSED\")\n",
                "    else:\n",
                "        print(\"SOME TESTS FAILED\")\n",
                "    \n",
                "    return all_passed\n",
                "\n",
                "e2e_passed = test_e2e_inference()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Final Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"ZENITH ADVANCED FEATURES - VALIDATION SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "results = {\n",
                "    \"CUDA Graphs\": cuda_graphs_passed,\n",
                "    \"Gradient Checkpointing Phase 1\": phase1_passed,\n",
                "    \"Gradient Checkpointing Phase 2 (Native)\": phase2_passed,\n",
                "    \"Complete E2E Inference\": e2e_passed,\n",
                "}\n",
                "\n",
                "for name, passed in results.items():\n",
                "    if passed is None:\n",
                "        status = \"SKIPPED\"\n",
                "    elif passed:\n",
                "        status = \"PASSED\"\n",
                "    else:\n",
                "        status = \"FAILED\"\n",
                "    print(f\"{name}: {status}\")\n",
                "\n",
                "total_passed = sum(1 for p in results.values() if p is True)\n",
                "total_failed = sum(1 for p in results.values() if p is False)\n",
                "total_skipped = sum(1 for p in results.values() if p is None)\n",
                "\n",
                "print()\n",
                "print(f\"Total: {total_passed} passed, {total_failed} failed, {total_skipped} skipped\")\n",
                "\n",
                "if total_failed == 0:\n",
                "    print(\"\\n*** ALL VALIDATIONS SUCCESSFUL ***\")\n",
                "else:\n",
                "    print(\"\\n*** SOME VALIDATIONS FAILED - NEEDS INVESTIGATION ***\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}