{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Comprehensive Verification\n",
                "\n",
                "**Purpose:** Verify Zenith on GPU with TensorFlow, JAX, and CUDA kernels\n",
                "\n",
                "**Requirements:**\n",
                "- Google Colab with GPU runtime\n",
                "- Change Runtime > GPU (T4 or better)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "!nvidia-smi\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Zenith\n",
                "!pip install pyzenith -q\n",
                "!pip install tensorflow jax jaxlib -q\n",
                "\n",
                "import zenith\n",
                "print(f\"Zenith Version: {zenith.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. PyTorch + CUDA Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import zenith\n",
                "from zenith.torch import compile as ztorch_compile\n",
                "import time\n",
                "\n",
                "# Simple model\n",
                "class SimpleModel(torch.nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = torch.nn.Linear(512, 1024)\n",
                "        self.fc2 = torch.nn.Linear(1024, 512)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.relu(self.fc1(x))\n",
                "        return self.fc2(x)\n",
                "\n",
                "model = SimpleModel().cuda()\n",
                "x = torch.randn(32, 512).cuda()\n",
                "\n",
                "# Baseline\n",
                "with torch.no_grad():\n",
                "    torch.cuda.synchronize()\n",
                "    start = time.perf_counter()\n",
                "    for _ in range(100):\n",
                "        _ = model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    baseline_time = time.perf_counter() - start\n",
                "\n",
                "print(f\"Baseline: {baseline_time*1000:.2f} ms\")\n",
                "\n",
                "# With torch.compile + Zenith backend\n",
                "try:\n",
                "    compiled = torch.compile(model, backend='zenith')\n",
                "    with torch.no_grad():\n",
                "        torch.cuda.synchronize()\n",
                "        start = time.perf_counter()\n",
                "        for _ in range(100):\n",
                "            _ = compiled(x)\n",
                "        torch.cuda.synchronize()\n",
                "        zenith_time = time.perf_counter() - start\n",
                "    \n",
                "    print(f\"Zenith: {zenith_time*1000:.2f} ms\")\n",
                "    print(f\"Speedup: {baseline_time/zenith_time:.2f}x\")\n",
                "    print(\"\\n✓ PyTorch + CUDA: VERIFIED\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ PyTorch + CUDA: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. CUDA Kernels Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check Zenith native CUDA kernels\n",
                "print(\"=== CUDA Kernel Registry ===\")\n",
                "\n",
                "try:\n",
                "    from zenith.runtime import KernelRegistry\n",
                "    \n",
                "    registry = KernelRegistry()\n",
                "    registry.initialize()\n",
                "    \n",
                "    print(f\"Initialized: {registry.is_initialized}\")\n",
                "    print(f\"Available kernels:\")\n",
                "    \n",
                "    for op in ['MatMul', 'ReLU', 'GELU', 'Softmax', 'LayerNorm']:\n",
                "        kernel = registry.get_kernel(op)\n",
                "        status = '✓' if kernel else '✗'\n",
                "        print(f\"  {status} {op}\")\n",
                "    \n",
                "    print(\"\\n✓ CUDA Kernels: VERIFIED\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ CUDA Kernels: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. TensorFlow Adapter Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== TensorFlow Adapter ===\")\n",
                "\n",
                "try:\n",
                "    import tensorflow as tf\n",
                "    from zenith.adapters import TensorFlowAdapter\n",
                "    \n",
                "    print(f\"TensorFlow: {tf.__version__}\")\n",
                "    print(f\"GPU Devices: {tf.config.list_physical_devices('GPU')}\")\n",
                "    \n",
                "    # Create simple Keras model\n",
                "    model = tf.keras.Sequential([\n",
                "        tf.keras.layers.Dense(256, activation='relu'),\n",
                "        tf.keras.layers.Dense(128, activation='relu'),\n",
                "        tf.keras.layers.Dense(10)\n",
                "    ])\n",
                "    model.build((None, 64))\n",
                "    \n",
                "    # Test adapter\n",
                "    adapter = TensorFlowAdapter()\n",
                "    print(f\"Adapter available: {adapter.is_available}\")\n",
                "    \n",
                "    # Convert to GraphIR\n",
                "    sample_input = tf.random.normal((1, 64))\n",
                "    graph_ir = adapter.from_model(model, sample_input)\n",
                "    \n",
                "    print(f\"GraphIR nodes: {graph_ir.num_nodes()}\")\n",
                "    print(\"\\n✓ TensorFlow Adapter: VERIFIED\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ TensorFlow Adapter: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. JAX Adapter Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== JAX Adapter ===\")\n",
                "\n",
                "try:\n",
                "    import jax\n",
                "    import jax.numpy as jnp\n",
                "    from zenith.adapters import JAXAdapter\n",
                "    \n",
                "    print(f\"JAX: {jax.__version__}\")\n",
                "    print(f\"JAX Devices: {jax.devices()}\")\n",
                "    \n",
                "    # Simple JAX function\n",
                "    def simple_mlp(x):\n",
                "        x = jnp.tanh(x @ jnp.ones((64, 128)))\n",
                "        x = jnp.tanh(x @ jnp.ones((128, 10)))\n",
                "        return x\n",
                "    \n",
                "    # Test adapter\n",
                "    adapter = JAXAdapter()\n",
                "    print(f\"Adapter available: {adapter.is_available}\")\n",
                "    \n",
                "    # Convert to GraphIR\n",
                "    sample_input = jnp.ones((1, 64))\n",
                "    graph_ir = adapter.from_model(simple_mlp, sample_input)\n",
                "    \n",
                "    print(f\"GraphIR nodes: {graph_ir.num_nodes()}\")\n",
                "    print(\"\\n✓ JAX Adapter: VERIFIED\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ JAX Adapter: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Benchmark on GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== GPU Benchmark ===\")\n",
                "\n",
                "try:\n",
                "    from benchmarks.mlperf_suite import (\n",
                "        ZenithBenchmark, BenchmarkConfig, generate_results_table\n",
                "    )\n",
                "    import torch\n",
                "    \n",
                "    # Model\n",
                "    model = torch.nn.Sequential(\n",
                "        torch.nn.Linear(256, 512),\n",
                "        torch.nn.ReLU(),\n",
                "        torch.nn.Linear(512, 256),\n",
                "    ).cuda()\n",
                "    \n",
                "    def model_fn(x):\n",
                "        with torch.no_grad():\n",
                "            return model(x)\n",
                "    \n",
                "    def input_gen(batch_size, seq_len):\n",
                "        return torch.randn(batch_size, 256).cuda()\n",
                "    \n",
                "    config = BenchmarkConfig(\n",
                "        model_name='MLP-256',\n",
                "        batch_sizes=[1, 8, 32],\n",
                "        num_warmup=10,\n",
                "        num_runs=100,\n",
                "        scenario='single-stream'\n",
                "    )\n",
                "    \n",
                "    benchmark = ZenithBenchmark(device='cuda')\n",
                "    results = benchmark.run(config, model_fn, input_gen)\n",
                "    \n",
                "    print(generate_results_table(results))\n",
                "    print(\"\\n✓ GPU Benchmark: VERIFIED\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ GPU Benchmark: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*50)\n",
                "print(\"ZENITH VERIFICATION SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print()\n",
                "print(f\"Zenith Version: {zenith.__version__}\")\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print()\n",
                "print(\"Components Verified:\")\n",
                "print(\"  - PyTorch Integration\")\n",
                "print(\"  - CUDA Kernels\")\n",
                "print(\"  - TensorFlow Adapter\")\n",
                "print(\"  - JAX Adapter\")\n",
                "print(\"  - MLPerf Benchmark Suite\")\n",
                "print()\n",
                "print(\"=\"*50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}