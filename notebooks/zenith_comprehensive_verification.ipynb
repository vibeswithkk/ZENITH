{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith 0.3.4 Verification\n",
                "\n",
                "**NEW in 0.3.4:** T4-optimized Triton autotune configs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. GPU Check\n",
                "!nvidia-smi | head -15\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"No GPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install 0.3.4 (auto-restart)\n",
                "!pip uninstall pyzenith -y 2>/dev/null\n",
                "!pip install pyzenith==0.3.4 --no-cache-dir -q\n",
                "import os; os.kill(os.getpid(), 9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Verify\n",
                "import zenith\n",
                "print(f\"Zenith: {zenith.__version__}\")\n",
                "assert zenith.__version__ == '0.3.4'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Triton Fused Kernel Benchmark\n",
                "from zenith.runtime.triton_kernels import benchmark_fused_linear_gelu, is_available\n",
                "\n",
                "print(f\"Triton: {is_available()}\")\n",
                "result = benchmark_fused_linear_gelu(M=1024, N=4096, K=1024, runs=50)\n",
                "print(f\"Fused: {result['fused_ms']:.2f}ms | Separate: {result['separate_ms']:.2f}ms | Speedup: {result['speedup']:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. torch.compile Benchmark\n",
                "import torch, time\n",
                "\n",
                "class MLP(torch.nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = torch.nn.Linear(512, 1024)\n",
                "        self.fc2 = torch.nn.Linear(1024, 512)\n",
                "    def forward(self, x):\n",
                "        return self.fc2(torch.relu(self.fc1(x)))\n",
                "\n",
                "model = MLP().cuda()\n",
                "x = torch.randn(32, 512).cuda()\n",
                "\n",
                "# Baseline\n",
                "with torch.no_grad():\n",
                "    for _ in range(10): model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    t0 = time.perf_counter()\n",
                "    for _ in range(100): model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    baseline = (time.perf_counter() - t0) * 1000\n",
                "\n",
                "# Zenith\n",
                "compiled = torch.compile(model, backend='zenith')\n",
                "with torch.no_grad():\n",
                "    for _ in range(10): compiled(x)\n",
                "    torch.cuda.synchronize()\n",
                "    t0 = time.perf_counter()\n",
                "    for _ in range(100): compiled(x)\n",
                "    torch.cuda.synchronize()\n",
                "    zenith_time = (time.perf_counter() - t0) * 1000\n",
                "\n",
                "print(f\"Baseline: {baseline:.2f}ms | Zenith: {zenith_time:.2f}ms | Speedup: {baseline/zenith_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Summary\n",
                "print(f\"\\nZenith {zenith.__version__} on {torch.cuda.get_device_name(0)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}