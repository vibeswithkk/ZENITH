{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith 0.3.5 - L2 Cache Optimized\n",
                "**NEW:** GROUP_SIZE_M block grouping for L2 cache locality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi | head -15\nimport torch\nprint(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip uninstall pyzenith -y 2>/dev/null\n!pip install pyzenith==0.3.5 --no-cache-dir -q\nimport os; os.kill(os.getpid(), 9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import zenith\nprint(f\"Version: {zenith.__version__}\")\nassert zenith.__version__ == '0.3.5'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fused Kernel Benchmark\nfrom zenith.runtime.triton_kernels import benchmark_fused_linear_gelu\n\nresult = benchmark_fused_linear_gelu(M=1024, N=4096, K=1024, runs=50)\nprint(f\"Fused: {result['fused_ms']:.2f}ms\")\nprint(f\"Separate: {result['separate_ms']:.2f}ms\")\nprint(f\"Speedup: {result['speedup']:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# torch.compile Benchmark\nimport torch\nimport time\n\nclass MLP(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(512, 1024)\n        self.fc2 = torch.nn.Linear(1024, 512)\n    def forward(self, x):\n        return self.fc2(torch.relu(self.fc1(x)))\n\nmodel = MLP().cuda()\nx = torch.randn(32, 512).cuda()\n\n# Baseline\nwith torch.no_grad():\n    for _ in range(10): model(x)\n    torch.cuda.synchronize()\n    t0 = time.perf_counter()\n    for _ in range(100): model(x)\n    torch.cuda.synchronize()\n    baseline = (time.perf_counter() - t0) * 1000\n\n# Zenith\ncompiled = torch.compile(model, backend='zenith')\nwith torch.no_grad():\n    for _ in range(10): compiled(x)\n    torch.cuda.synchronize()\n    t0 = time.perf_counter()\n    for _ in range(100): compiled(x)\n    torch.cuda.synchronize()\n    zenith_time = (time.perf_counter() - t0) * 1000\n\nprint(f\"Baseline: {baseline:.2f}ms | Zenith: {zenith_time:.2f}ms | Speedup: {baseline/zenith_time:.2f}x\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}