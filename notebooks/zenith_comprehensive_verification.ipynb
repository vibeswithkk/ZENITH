{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith 0.3.3 Verification\n",
                "\n",
                "Run all cells in order. After cell 2, runtime will restart automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. GPU Check\n",
                "!nvidia-smi | head -20\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"No GPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Zenith 0.3.3 (will restart runtime)\n",
                "!pip uninstall pyzenith -y 2>/dev/null\n",
                "!pip cache purge 2>/dev/null\n",
                "!pip install pyzenith==0.3.3 --no-cache-dir -q\n",
                "\n",
                "# Auto restart to clear cached imports\n",
                "import os\n",
                "os.kill(os.getpid(), 9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Verify Version\n",
                "import zenith\n",
                "print(f\"Zenith: {zenith.__version__}\")\n",
                "assert zenith.__version__ == '0.3.3', f\"Wrong version: {zenith.__version__}\"\n",
                "print(\"✓ Version OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Triton Check\n",
                "from zenith.runtime.triton_kernels import is_available, get_version, get_triton_kernel_map\n",
                "\n",
                "print(f\"Triton: {get_version()}\")\n",
                "print(f\"Available: {is_available()}\")\n",
                "if is_available():\n",
                "    print(f\"Kernels: {list(get_triton_kernel_map().keys())}\")\n",
                "    print(\"✓ Triton OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Triton Fused Kernel Benchmark\n",
                "from zenith.runtime.triton_kernels import benchmark_fused_linear_gelu\n",
                "\n",
                "result = benchmark_fused_linear_gelu(M=1024, N=4096, K=1024, runs=50)\n",
                "print(f\"Fused: {result['fused_ms']:.2f} ms\")\n",
                "print(f\"Separate: {result['separate_ms']:.2f} ms\")\n",
                "print(f\"Speedup: {result['speedup']:.2f}x\")\n",
                "\n",
                "if result['speedup'] > 1.0:\n",
                "    print(\"\\n✓ FUSED KERNEL FASTER!\")\n",
                "else:\n",
                "    print(\"\\n⚠ No speedup\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. torch.compile with Zenith\n",
                "import torch\n",
                "import time\n",
                "\n",
                "class MLP(torch.nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = torch.nn.Linear(512, 1024)\n",
                "        self.fc2 = torch.nn.Linear(1024, 512)\n",
                "    def forward(self, x):\n",
                "        return self.fc2(torch.relu(self.fc1(x)))\n",
                "\n",
                "model = MLP().cuda()\n",
                "x = torch.randn(32, 512).cuda()\n",
                "\n",
                "# Baseline\n",
                "with torch.no_grad():\n",
                "    for _ in range(10): model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    t0 = time.perf_counter()\n",
                "    for _ in range(100): model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    baseline = (time.perf_counter() - t0) * 1000\n",
                "\n",
                "# Zenith\n",
                "compiled = torch.compile(model, backend='zenith')\n",
                "with torch.no_grad():\n",
                "    for _ in range(10): compiled(x)\n",
                "    torch.cuda.synchronize()\n",
                "    t0 = time.perf_counter()\n",
                "    for _ in range(100): compiled(x)\n",
                "    torch.cuda.synchronize()\n",
                "    zenith_time = (time.perf_counter() - t0) * 1000\n",
                "\n",
                "print(f\"Baseline: {baseline:.2f} ms\")\n",
                "print(f\"Zenith: {zenith_time:.2f} ms\")\n",
                "print(f\"Speedup: {baseline/zenith_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Summary\n",
                "print(\"=\"*40)\n",
                "print(f\"Zenith {zenith.__version__} on {torch.cuda.get_device_name(0)}\")\n",
                "print(\"=\"*40)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}