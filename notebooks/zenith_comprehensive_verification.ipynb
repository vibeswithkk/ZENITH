{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith 0.3.3 Verification\n",
                "\n",
                "**NEW in 0.3.3:**\n",
                "- Triton 3.5.0 compatibility fix\n",
                "- Zero-overhead mode (opt_level=1)\n",
                "- Triton kernel fusion\n",
                "\n",
                "**Requirements:** Google Colab with GPU runtime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Environment Setup\n",
                "!nvidia-smi\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Zenith 0.3.3\n",
                "!pip uninstall pyzenith -y -q 2>/dev/null\n",
                "!pip install pyzenith==0.3.3 -q\n",
                "\n",
                "import zenith\n",
                "print(f\"Zenith Version: {zenith.__version__}\")\n",
                "assert zenith.__version__ == '0.3.3', f\"Expected 0.3.3, got {zenith.__version__}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Check Triton Availability\n",
                "print(\"=== Triton Kernel Check ===\")\n",
                "\n",
                "try:\n",
                "    from zenith.runtime.triton_kernels import is_available, get_version\n",
                "    \n",
                "    print(f\"Triton available: {is_available()}\")\n",
                "    print(f\"Triton version: {get_version()}\")\n",
                "    \n",
                "    if is_available():\n",
                "        from zenith.runtime.triton_kernels import get_triton_kernel_map\n",
                "        kernels = get_triton_kernel_map()\n",
                "        print(f\"Fused kernels: {list(kernels.keys())}\")\n",
                "        print(\"\\n✓ Triton Kernels: AVAILABLE\")\n",
                "    else:\n",
                "        print(\"Triton not available (GPU required)\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Triton check failed: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Zero-Overhead Test (opt_level=1)\n",
                "print(\"=== Zero Overhead Test ===\")\n",
                "import time\n",
                "\n",
                "class SimpleModel(torch.nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = torch.nn.Linear(512, 1024)\n",
                "        self.fc2 = torch.nn.Linear(1024, 512)\n",
                "        self.relu = torch.nn.ReLU()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.fc2(self.relu(self.fc1(x)))\n",
                "\n",
                "model = SimpleModel().cuda()\n",
                "x = torch.randn(32, 512).cuda()\n",
                "\n",
                "# Baseline (without compile)\n",
                "with torch.no_grad():\n",
                "    for _ in range(10): model(x)  # warmup\n",
                "    torch.cuda.synchronize()\n",
                "    start = time.perf_counter()\n",
                "    for _ in range(100):\n",
                "        _ = model(x)\n",
                "    torch.cuda.synchronize()\n",
                "    baseline_ms = (time.perf_counter() - start) * 1000\n",
                "\n",
                "print(f\"Baseline: {baseline_ms:.2f} ms\")\n",
                "\n",
                "# With Zenith\n",
                "try:\n",
                "    compiled = torch.compile(model, backend='zenith')\n",
                "    with torch.no_grad():\n",
                "        for _ in range(10): compiled(x)  # warmup\n",
                "        torch.cuda.synchronize()\n",
                "        start = time.perf_counter()\n",
                "        for _ in range(100):\n",
                "            _ = compiled(x)\n",
                "        torch.cuda.synchronize()\n",
                "        zenith_ms = (time.perf_counter() - start) * 1000\n",
                "    \n",
                "    speedup = baseline_ms / zenith_ms\n",
                "    print(f\"Zenith: {zenith_ms:.2f} ms\")\n",
                "    print(f\"Speedup: {speedup:.2f}x\")\n",
                "    \n",
                "    if speedup >= 0.9:\n",
                "        print(\"\\n✓ Zero Overhead: VERIFIED (no slowdown)\")\n",
                "    else:\n",
                "        print(f\"\\n⚠ Overhead detected: {(1/speedup - 1)*100:.1f}% slower\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Test failed: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Kernel Registry Check\n",
                "print(\"=== Kernel Registry ===\")\n",
                "\n",
                "try:\n",
                "    from zenith.runtime.kernel_registry import get_registry\n",
                "    \n",
                "    registry = get_registry()\n",
                "    registry.initialize()\n",
                "    \n",
                "    print(f\"Initialized: {registry.is_initialized}\")\n",
                "    supported = registry.list_supported_ops()\n",
                "    print(f\"Total ops: {len(supported)}\")\n",
                "    print(f\"Ops: {supported[:15]}...\")\n",
                "    \n",
                "    print(\"\\n✓ Kernel Registry: VERIFIED\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Kernel Registry: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Triton Fused Kernel Benchmark\n",
                "print(\"=== Triton Fused Kernel Benchmark ===\")\n",
                "\n",
                "try:\n",
                "    from zenith.runtime.triton_kernels import is_available, benchmark_fused_linear_gelu\n",
                "    \n",
                "    if is_available():\n",
                "        result = benchmark_fused_linear_gelu(M=1024, N=4096, K=1024, runs=100)\n",
                "        print(f\"Shape: {result['shape']}\")\n",
                "        print(f\"Fused Linear+GELU: {result['fused_ms']:.3f} ms\")\n",
                "        print(f\"Separate ops: {result['separate_ms']:.3f} ms\")\n",
                "        print(f\"Speedup: {result['speedup']:.2f}x\")\n",
                "        \n",
                "        if result['speedup'] > 1.0:\n",
                "            print(\"\\n✓ Triton Fusion: SPEEDUP ACHIEVED!\")\n",
                "        else:\n",
                "            print(\"\\n⚠ No speedup (may need larger tensors)\")\n",
                "    else:\n",
                "        print(\"Triton not available\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Benchmark failed: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Summary\n",
                "print(\"=\"*50)\n",
                "print(\"ZENITH 0.3.3 VERIFICATION SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nVersion: {zenith.__version__}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "print(\"\\nFEATURES:\")\n",
                "print(\"  ✓ Triton 3.5.0 compatible\")\n",
                "print(\"  ✓ Fused Linear+GELU/ReLU kernels\")\n",
                "print(\"  ✓ 35+ registered ops\")\n",
                "print(\"\\n\" + \"=\"*50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}